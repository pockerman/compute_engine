{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 38: Uncostrained Optimization With Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [Overview](#overview) \n",
    "    * [Uncostrained optimization with Gradient Descent](#ekf)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)\n",
    "* [References](#refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest algorithm for uncostrained optimization is <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\"> gradient descent</a> also known as steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Uncostrained optimization with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following function [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\theta_1, \\theta_2) = \\frac{1}{2}(\\theta_{1}^2 - \\theta_2)^2 + \\frac{1}{2}(\\theta_1 -1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding $\\theta_1, \\theta_2$ that minimize $f$. Gradient descent is an iterative algorithm that uses the gradient of the function in order to update the parameters. The update rule is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1} - \\eta \\nabla f|_{\\boldsymbol{\\theta}_{k-1}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta$ is the so called learning rate and tunes how fast we move to the direction of the gradient. A small $\\eta$ slows down convergence whilst a large value may not allow convergence of the algorithm. This is shown in the two figures below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "SGD: iteration: 1 eta: 0.1\n",
    "\tAbsolute Total Error: 43.3685 Tol: 1e-08\n",
    "SGD: iteration: 2 eta: 0.1\n",
    "\tAbsolute Total Error: 14.6777 Tol: 1e-08\n",
    "SGD: iteration: 3 eta: 0.1\n",
    "\tAbsolute Total Error: 18.475 Tol: 1e-08\n",
    "SGD: iteration: 4 eta: 0.1\n",
    "\tAbsolute Total Error: 3.04793 Tol: 1e-08\n",
    "SGD: iteration: 5 eta: 0.1\n",
    "\tAbsolute Total Error: 0 Tol: 1e-08\n",
    "# iterations:..5\n",
    "# processors:..1\n",
    "# threads:.....1\n",
    "Residual:......0\n",
    "Tolerance:.....1e-08\n",
    "Convergence:...Yes\n",
    "Total time:....0.000479557\n",
    "Learning rate:..0.1\n",
    "\n",
    "Predicted: 0 expected: 0\n",
    "Predicted: 0 expected: 0\n",
    "Predicted: 0 expected: 0\n",
    "Predicted: 0 expected: 0\n",
    "Predicted: 0 expected: 0\n",
    "Predicted: 1 expected: 1\n",
    "Predicted: 1 expected: 1\n",
    "Predicted: 1 expected: 1\n",
    "Predicted: 1 expected: 1\n",
    "Predicted: 1 expected: 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kevin P. Murphy, ```Machine Learning A Probabilistic Perspective```, The MIT Press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
