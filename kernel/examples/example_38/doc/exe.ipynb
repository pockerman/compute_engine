{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 38: Uncostrained Optimization With Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [Overview](#overview) \n",
    "    * [Uncostrained optimization with Gradient Descent](#ekf)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source code](#source_code)\n",
    "* [References](#refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest algorithm for uncostrained optimization is <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\"> gradient descent</a> also known as steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Uncostrained optimization with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following function [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\theta_1, \\theta_2) = \\frac{1}{2}(\\theta_{1}^2 - \\theta_2)^2 + \\frac{1}{2}(\\theta_1 -1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding $\\theta_1, \\theta_2$ that minimize $f$. Gradient descent is an iterative algorithm that uses the gradient of the function in order to update the parameters. The update rule is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1} - \\eta \\nabla f|_{\\boldsymbol{\\theta}_{k-1}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta$ is the so called learning rate and tunes how fast we move to the direction of the gradient. A small $\\eta$ slows down convergence whilst a large value may not allow convergence of the algorithm. This is shown in the two figures below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"gd_1.png\" alt=\"Gradient descent eta 0.1\"\n",
    "\ttitle=\"Gradient descent eta 0.1\" width=\"400\" height=\"350\" />\n",
    "<figcaption>Figure: Gradient descent with eta 0.1.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"gd_2.png\" alt=\"Gradient descent eta 0.6\"\n",
    "\ttitle=\"Gradient descent eta 0.6\" width=\"400\" height=\"350\" />\n",
    "<figcaption>Figure: Gradient descent with eta 0.6.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"include_files\"></a> Include files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include \"kernel/base/config.h\"\n",
    "#include \"kernel/base/types.h\"\n",
    "#include \"kernel/base/kernel_consts.h\"\n",
    "#include \"kernel/utilities/common_uitls.h\"\n",
    "#include \"kernel/maths/optimization/serial_gradient_descent.h\"\n",
    "#include \"kernel/maths/functions/function_base.h\"\n",
    "\n",
    "#include <iostream>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "namespace example {\n",
    "\n",
    "using kernel::real_t;\n",
    "using kernel::uint_t;\n",
    "using kernel::DynMat;\n",
    "using kernel::DynVec;\n",
    "using kernel::maths::opt::Gd;\n",
    "using kernel::maths::opt::GDConfig;\n",
    "\n",
    "class Function: public kernel::FunctionBase<real_t, DynVec<real_t>>\n",
    "{\n",
    "public:\n",
    "\n",
    "    typedef kernel::FunctionBase<real_t, DynVec<real_t>>::output_t output_t;\n",
    "\n",
    "    // constructor\n",
    "    Function(const DynVec<real_t>& coeffs);\n",
    "\n",
    "    // compute the value of the function\n",
    "    virtual output_t value(const DynVec<real_t>&  input)const override final;\n",
    "\n",
    "    // compute the gradients of the function\n",
    "    virtual DynVec<real_t> gradients(const DynVec<real_t>&  input)const override final;\n",
    "\n",
    "    // the number of coefficients\n",
    "    virtual uint_t n_coeffs()const override final{return 2;}\n",
    "\n",
    "    // reset the coefficients\n",
    "    void set_coeffs(const DynVec<real_t>&  coeffs){coeffs_ = coeffs;}\n",
    "\n",
    "    // get a copy of the coefficients\n",
    "    DynVec<real_t> coeffs()const{return coeffs_;}\n",
    "\n",
    "private:\n",
    "\n",
    "    // coefficients vector\n",
    "    DynVec<real_t> coeffs_;\n",
    "\n",
    "};\n",
    "\n",
    "Function::Function(const DynVec<real_t>& coeffs)\n",
    "    :\n",
    "      coeffs_(coeffs)\n",
    "{}\n",
    "\n",
    "Function::output_t\n",
    "Function::value(const DynVec<real_t>&  input)const{\n",
    "\n",
    "    std::cout<<\"th1 \"<<input[0]<<\" th2 \"<<input[1]<<std::endl;\n",
    "    return 0.5*(kernel::utils::sqr(kernel::utils::sqr(input[0]) - input[1])) +\n",
    "           0.5*(kernel::utils::sqr(input[0] - 1.0));\n",
    "}\n",
    "\n",
    "DynVec<real_t>\n",
    "Function::gradients(const DynVec<real_t>&  input)const{\n",
    "\n",
    "    auto grad1= 2.0*input[0]*(kernel::utils::sqr(input[0]) - input[1]) + (input[0] - 1.0);\n",
    "    auto grad2 = -(kernel::utils::sqr(input[0]) - input[1]);\n",
    "    DynVec<real_t> rslt(2, 0.0);\n",
    "    rslt[0] = grad1;\n",
    "    rslt[1] = grad2;\n",
    "    return rslt;\n",
    "}\n",
    "\n",
    "}\n",
    "\n",
    "int main(){\n",
    "\n",
    "    using namespace example;\n",
    "    try{\n",
    "\n",
    "        GDConfig config(10, kernel::KernelConsts::tolerance(), 0.1);\n",
    "        config.set_show_iterations_flag(true);\n",
    "        Gd gd(config);\n",
    "\n",
    "        DynVec<real_t> coeffs(2, 0.0);\n",
    "\n",
    "        Function f(coeffs);\n",
    "\n",
    "        auto info = gd.solve(f);\n",
    "        std::cout<<info<<std::endl;\n",
    "\n",
    "    }\n",
    "    catch(std::logic_error& error){\n",
    "\n",
    "        std::cerr<<error.what()<<std::endl;\n",
    "    }\n",
    "    catch(...){\n",
    "        std::cerr<<\"Unknown exception occured\"<<std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">GD: iteration: 1\n",
    "\t eta: 0.1 ABS error: 0.09495 Exit Tol: 1e-08\n",
    ">GD: iteration: 2\n",
    "\t eta: 0.1 ABS error: 0.0762246 Exit Tol: 1e-08\n",
    ">GD: iteration: 3\n",
    "\t eta: 0.1 ABS error: 0.0596829 Exit Tol: 1e-08\n",
    ">GD: iteration: 4\n",
    "\t eta: 0.1 ABS error: 0.0452378 Exit Tol: 1e-08\n",
    ">GD: iteration: 5\n",
    "\t eta: 0.1 ABS error: 0.0333377 Exit Tol: 1e-08\n",
    ">GD: iteration: 6\n",
    "\t eta: 0.1 ABS error: 0.0242542 Exit Tol: 1e-08\n",
    ">GD: iteration: 7\n",
    "\t eta: 0.1 ABS error: 0.0178224 Exit Tol: 1e-08\n",
    ">GD: iteration: 8\n",
    "\t eta: 0.1 ABS error: 0.0135337 Exit Tol: 1e-08\n",
    ">GD: iteration: 9\n",
    "\t eta: 0.1 ABS error: 0.0107686 Exit Tol: 1e-08\n",
    ">GD: iteration: 10\n",
    "\t eta: 0.1 ABS error: 0.00898365 Exit Tol: 1e-08\n",
    ">GD: iteration: 11\n",
    "\t eta: 0.1 ABS error: 0.00778693 Exit Tol: 1e-08\n",
    ">GD: iteration: 12\n",
    "\t eta: 0.1 ABS error: 0.00693079 Exit Tol: 1e-08\n",
    ">GD: iteration: 13\n",
    "\t eta: 0.1 ABS error: 0.0062723 Exit Tol: 1e-08\n",
    ">GD: iteration: 14\n",
    "\t eta: 0.1 ABS error: 0.00573354 Exit Tol: 1e-08\n",
    ">GD: iteration: 15\n",
    "\t eta: 0.1 ABS error: 0.00527306 Exit Tol: 1e-08\n",
    ">GD: iteration: 16\n",
    "\t eta: 0.1 ABS error: 0.00486855 Exit Tol: 1e-08\n",
    ">GD: iteration: 17\n",
    "\t eta: 0.1 ABS error: 0.00450745 Exit Tol: 1e-08\n",
    ">GD: iteration: 18\n",
    "\t eta: 0.1 ABS error: 0.00418203 Exit Tol: 1e-08\n",
    ">GD: iteration: 19\n",
    "\t eta: 0.1 ABS error: 0.00388703 Exit Tol: 1e-08\n",
    ">GD: iteration: 20\n",
    "\t eta: 0.1 ABS error: 0.00361855 Exit Tol: 1e-08\n",
    "# iterations:..20\n",
    "# processors:..1\n",
    "# threads:.....1\n",
    "Residual:......0.00361855\n",
    "Tolerance:.....1e-08\n",
    "Convergence:...No\n",
    "Total time:....0.000364831\n",
    "Learning rate:..0.1\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exe.cpp</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kevin P. Murphy, ```Machine Learning A Probabilistic Perspective```, The MIT Press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
