{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 25: Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Acknowledgements](#ackw)\n",
    "* [Overview](#overview) \n",
    "    * [Iterative policy evaluation](#ekf)\n",
    "    * [Test case](#test_case)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ackw\"></a> Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the content in this notebook is taken from the book ```Reinforcement Learning An introduction``` by Sutton and Barto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we go over another classical reinforcement learning algorithm; namely iterative policy evaluation. This is a method for computing an optimal policy $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Iterative policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative policy evaluation is actually a dynamic programming algorithm. With these algorithms, one should supply the model of the environment. As such, one can argue that classical DP algorithms are probably of limited utility in the field of reinforcement learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will implement a table based approach to iterative policy evaluation. Therefore, we will have to assume that the state, $S$, action $A$ and reward, $R$, sets are finite. Furthermore, we will assume that the dynamics can be described by a set of probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(s^{'}, r | s, \\alpha), ~~ \\forall s  \\in S, \\alpha \\in A, r \\in R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily obtain optimal policies once we have found the optimal functions that is either $V^{*}$ or $Q^{*}$. We know that these will satisfy respectively the following Bellman equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V^{*}(s) = max_{\\alpha} E[R_{t+1} + \\gamma V^{*}(s_{t+1}) | S_t = s, A_t = \\alpha]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q^{*}(s,\\alpha)= E[R_{t+1} + \\gamma max_{\\alpha^{'}}Q^{*}(s_{t+1}, \\alpha^{'}) | S_t = s, A_t = \\alpha]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP algorithms can be obtained by turning Bellman equations, such as the ones above, into assignments. As we will see below, these are just update rules for improving approximations of the desired value functions. Let's see how to compute the state value function $V_{\\pi}$ for an arbitrary policy $\\pi$.  This is called policy evaluation. Concretely, since the method is iterative is called iterative policy evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the following for the value function $V_{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{\\pi}(s) = E_{\\pi}[G_t| S_t = s] = \\sum_{\\alpha} \\pi(\\alpha |s) \\sum_{s^{'}, r} p(s^{'}, r | s, \\alpha)[r + \\gamma V_{\\pi}(s^{'})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\pi(\\alpha |s)$ is the probability of taking action $\\alpha$ whilst in state $s$ under the policy $\\pi$. If the dynamics of the environment, $p(s^{'}, r | s, \\alpha)$, are known, then the equation above is a system of $|S|$ simulataneous equations. The unknowns are the $V_{\\pi}(s), s \\in S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the system is linear, its solution is  straightforward. All we need to do is to inverte the system matrix. The latter however is not always easy particular if the state space is large. We can avoid matrix inversion altogether by using iterative solution methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a sequence of approximate value functions  $V_0, V_1,V_2, \\cdots$ \n",
    "The initial approximation, $V_0$ , is chosen arbitrarily (except for the terminal state whic is given the value 0). Each successive approximation is obtained by using the Bellman equation for $V$ as an update rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{k+1}(s) = \\sum_{\\alpha} \\pi(\\alpha |s) \\sum_{s^{'}, r} p(s^{'}, r | s, \\alpha)[r + \\gamma V_{k}(s^{'})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_k = V_{\\pi}$ is a fixed point for this update rule. The sequence ${V_k}$ can be shown to converge to $V_{\\pi}$ as $k\\rightarrow \\infty$. This algorithm is called iterative policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce each successive approximation, $V_{k+1}$ from $V_k$, iterative policy evaluation\n",
    "applies the same operation to each state $s$: it replaces the old value of $s$ with a new value\n",
    "obtained from the old values of the successor states of $s$, and the expected immediate\n",
    "rewards, along all the one-step transitions possible under the policy being evaluated. We\n",
    "call this kind of operation an expected update. Each iteration of iterative policy evaluation updates the value of every state once to produce the new approximate value function $V_{k+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A  simple implementation of iterative policy evaluation, will use two arrays one for  $V_{k}(s)$, and one for $V_{k+1}(s)$. With two arrays, the new values can be computed one by one from the old values without the old values being changed. In pseudocode the algorithm looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"iterative_policy_evaluation.png\"\n",
    "     alt=\"Iterative Policy Evaluation Algorithm\"\n",
    "     style=\"float: left; margin-right: 10px; width: 500px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the image is from the book ```Reinforcement Learning An introduction``` by Sutton and Barto.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use one array and update the values in place, that is, with each new value immediately\n",
    "overwriting the old one. Then, depending on the order in which the states are updated,\n",
    "sometimes new values are used instead of old ones on the right-hand side of the equation above. This\n",
    "in-place algorithm also converges to $V_{\\pi}$; in fact, it usually converges faster than the\n",
    "two-array version, as you might expect, because it uses new data as soon as they are\n",
    "available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for  the in-place algorithm, the order in which states have their values updated during the\n",
    "sweep has a significant influence on the rate of convergence. For this reason the class ```SyncValueFuncItr``` uses a two arrays implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the implementation approach used, iterative policy evaluation improves a given initial policy. This is shown schematically in the figure below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"grid_world_policy.png\"\n",
    "     alt=\"Grid World Policy\"\n",
    "     style=\"float: left; margin-right: 10px; width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"test_case\"></a> Test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will simulate an episodic MDP. The world is a square $4 \\times 4$ grid. The goal states are the top left and bottom right cells. The initial $V_{\\pi}$ function is shown in the image below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"initial_grid_world.png\"\n",
    "     alt=\"Initial Grid World\"\n",
    "     style=\"float: left; margin-right: 10px; width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each state can have four possible actions; ```UP```, ```DOWN```, ```LEFT``` and ```RIGHT```. The code below uses the ```cengine::rl::worlds::GridWorldAction``` enumeration to describe that. If the agent goes off the world it is assumed to come back  on the  state that led it outside. For every transition we will assume a reward of $R=-1$. We also use $\\gamma=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics function $p(s^{'}, r | s, \\alpha)$ is modeled via the following lambda expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " auto dynamics = [](const state_t& s1, real_t,\n",
    "                const state_t& s2, const action_t& action){\n",
    "          0.25\n",
    "        };\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we return this value and not 1? Afterall, the dynamics is deterministic. This is because the code loops over the transition states for state $s$ regardless of the action chosen. However, the agent cannot transition from every state to another state regardless of the action. In other words, the function $p(s^{'}, r | s, \\alpha)$ should return zero as well. In order not to clutter the code with if/else statements we use this workaround.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the function $\\pi(\\alpha, s)$ is modeled by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " auto policy = [](const action_t&, const state_t&){\n",
    "          return 0.25;\n",
    "        };\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is under the given policy, the agent has an equal probability to select any of the allowed four actions. The value function on some iterations is shown in the figure below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gird_world_value_function.png\"\n",
    "     alt=\"Grid World Value Function\"\n",
    "     style=\"float: left; margin-right: 10px; width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"include_files\"></a> Include files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include \"cubic_engine/base/cubic_engine_types.h\"\n",
    "#include \"kernel/base/kernel_consts.h\"\n",
    "#include \"cubic_engine/rl/worlds/grid_world.h\"\n",
    "#include \"cubic_engine/rl/worlds/grid_world_action_space.h\"\n",
    "#include \"cubic_engine/rl/synchronous_value_function_learning.h\"\n",
    "#include \"cubic_engine/rl/reward_table.h\"\n",
    "\n",
    "#include <iostream>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "namespace example\n",
    "{\n",
    "\n",
    "using cengine::uint_t;\n",
    "using cengine::real_t;\n",
    "using cengine::rl::worlds::GridWorld;\n",
    "using cengine::rl::worlds::GridWorldAction;\n",
    "using cengine::rl::SyncValueFuncItr;\n",
    "using cengine::rl::SyncValueFuncItrInput;\n",
    "using cengine::rl::RewardTable;\n",
    "using kernel::CSVWriter;\n",
    "\n",
    "class RewardProducer\n",
    "{\n",
    "public:\n",
    "\n",
    "    typedef real_t value_t;\n",
    "\n",
    "    /// construcotr\n",
    "    RewardProducer();\n",
    "\n",
    "    /// returns the reward for the goal\n",
    "    real_t goal_reward()const{return 0.0;}\n",
    "\n",
    "    /// returns the reward for the action\n",
    "    /// at  state s when going to state sprime\n",
    "    template<typename ActionTp, typename StateTp>\n",
    "    real_t get_reward(const ActionTp& action,\n",
    "                      const StateTp& s,\n",
    "                      const StateTp& sprime)const{\n",
    "        return rewards_.get_reward(s.get_id(), action);\n",
    "    }\n",
    "\n",
    "    /// returns the reward for the action\n",
    "    /// at  state s when going to state sprime\n",
    "     template<typename ActionTp, typename StateTp>\n",
    "     real_t get_reward(const ActionTp& action,\n",
    "                          const StateTp& s)const{\n",
    "            return rewards_.get_reward(s.get_id(), action);\n",
    "     }\n",
    "\n",
    "private:\n",
    "\n",
    "    /// table that holds the rewards\n",
    "    RewardTable<GridWorldAction, real_t> rewards_;\n",
    "\n",
    "    /// setup the rewards\n",
    "    void setup_rewards();\n",
    "};\n",
    "\n",
    "RewardProducer::RewardProducer()\n",
    "    :\n",
    "   rewards_()\n",
    "{\n",
    "    setup_rewards();\n",
    "}\n",
    "\n",
    "void\n",
    "RewardProducer::setup_rewards(){\n",
    "\n",
    "    rewards_.set_reward(0, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(0, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(0, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(0, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(1, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(1, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(1, GridWorldAction::WEST, -1.0);\n",
    "    rewards_.set_reward(1, GridWorldAction::SOUTH, -1.0);\n",
    "\n",
    "    rewards_.set_reward(2, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(2, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(2, GridWorldAction::WEST, -1.0);\n",
    "    rewards_.set_reward(2, GridWorldAction::SOUTH, -1.0);\n",
    "\n",
    "    rewards_.set_reward(4, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(4, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(4, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(4, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(5, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(5, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(5, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(5, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(6, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(6, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(6, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(6, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(7, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(7, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(7, GridWorldAction::WEST, -1.0);\n",
    "    rewards_.set_reward(7, GridWorldAction::EAST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(8, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(8, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(8, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(8, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(9, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(9, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(9, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(9, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(10, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(10, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(10, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(10, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(11, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(11, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(11, GridWorldAction::WEST, -1.0);\n",
    "    rewards_.set_reward(11, GridWorldAction::EAST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(13, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(13, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(13, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(13, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(14, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(14, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(14, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(14, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(15, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(15, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(15, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(15, GridWorldAction::WEST, -1.0);\n",
    "}\n",
    "\n",
    "typedef GridWorld<RewardProducer> world_t;\n",
    "typedef world_t::state_t state_t;\n",
    "\n",
    "const uint_t N_CELLS = 4;\n",
    "\n",
    "void\n",
    "create_wolrd(world_t& w){\n",
    "\n",
    "   std::vector<state_t> world_states;\n",
    "   world_states.reserve(N_CELLS*N_CELLS);\n",
    "\n",
    "   uint_t counter=0;\n",
    "   for(uint_t i=0; i<N_CELLS; ++i){\n",
    "       for(uint_t j=0; j<N_CELLS; ++j){\n",
    "           world_states.push_back(state_t(counter++));\n",
    "       }\n",
    "   }\n",
    "\n",
    "   w.set_states(std::move(world_states));\n",
    "\n",
    "   counter=0;\n",
    "   for(uint_t i=0; i<N_CELLS*N_CELLS; ++i){\n",
    "\n",
    "       auto& state = w.get_state(i);\n",
    "\n",
    "       /// bottom row\n",
    "       if(i <4){\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), &state);\n",
    "\n",
    "           if(i != 3){\n",
    "             state.set_transition(GridWorldAction::EAST, &w.get_state(i+1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(GridWorldAction::EAST, &state);\n",
    "           }\n",
    "\n",
    "           state.set_transition(GridWorldAction::NORTH, &w.get_state(N_CELLS + i));\n",
    "\n",
    "           if(i == 0){\n",
    "                state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &state);\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "       }\n",
    "       else if(i >= 12 ){\n",
    "           /// top row\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), &w.get_state(i - N_CELLS));\n",
    "\n",
    "           if(i != 15){\n",
    "             state.set_transition(GridWorldAction::EAST, &w.get_state(i+1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(GridWorldAction::EAST, &state);\n",
    "           }\n",
    "\n",
    "           state.set_transition(GridWorldAction::NORTH, &state);\n",
    "\n",
    "           if(i == 12){\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &state);\n",
    "           }\n",
    "           else{\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "       }\n",
    "       else{\n",
    "\n",
    "           /// all rows in between\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), &w.get_state(i - N_CELLS));\n",
    "\n",
    "           if(i != 11 && i != 7){\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::EAST), &w.get_state(i +1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::EAST), &state);\n",
    "           }\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::NORTH), &w.get_state(i + N_CELLS));\n",
    "\n",
    "           if(i != 4 && i != 8 ){\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "           else {\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &state);\n",
    "           }\n",
    "       }\n",
    "   }\n",
    "}\n",
    "\n",
    "}\n",
    "\n",
    "int main(){\n",
    "\n",
    "    using namespace example;\n",
    "\n",
    "    try{\n",
    "\n",
    "        typedef GridWorld<RewardProducer> world_t;\n",
    "        typedef world_t::state_t state_t;\n",
    "        typedef world_t::action_t action_t;\n",
    "\n",
    "        auto policy = [](const action_t&, const state_t&){\n",
    "          return 0.25;\n",
    "        };\n",
    "\n",
    "        auto dynamics = [](const state_t& s1, real_t,\n",
    "                const state_t& s2, const action_t& action){\n",
    "          return 0.25;\n",
    "        };\n",
    "\n",
    "        std::vector<real_t> rewards(1, -1.0);\n",
    "\n",
    "        /// the world of the agent\n",
    "        world_t world;\n",
    "        create_wolrd(world);\n",
    "\n",
    "\n",
    "        std::cout<<\"Number of states: \"<<world.n_states()<<std::endl;\n",
    "\n",
    "        state_t start(15);\n",
    "        state_t goal1(3);\n",
    "        state_t goal2(12);\n",
    "\n",
    "        world.append_goal(goal1);\n",
    "        world.append_goal(goal2);\n",
    "\n",
    "        /// simulation parameters\n",
    "        /// number of episodes for the agent to learn.\n",
    "        const uint_t N_ITERATIONS = 160;\n",
    "        const real_t TOL = 0.001;\n",
    "        const real_t GAMMA = 1.0;\n",
    "\n",
    "        SyncValueFuncItrInput input={TOL, GAMMA, N_ITERATIONS, true};\n",
    "        SyncValueFuncItr<world_t> learner(std::move(input));\n",
    "\n",
    "        std::vector<real_t> row(2);\n",
    "        learner.initialize(world, 0.0);\n",
    "\n",
    "        world.restart(start);\n",
    "\n",
    "        while(learner.continue_iterations()){\n",
    "\n",
    "            std::cout<<\"At iteration: \"<<learner.get_current_iteration()<<std::endl;\n",
    "\n",
    "            learner.step(policy, dynamics);\n",
    "            auto values = learner.get_values();\n",
    "\n",
    "            for(auto c=0; c<values.size(); ++c){\n",
    "                std::cout<<\"Cell: \"<<c<<\" value: \"<<values[c]<<std::endl;\n",
    "            }   \n",
    "        }    \n",
    "    }\n",
    "    catch(std::exception& e){\n",
    "\n",
    "        std::cerr<<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(...){\n",
    "\n",
    "        std::cerr<<\"Unknown exception occured\"<<std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Number of states: 16\n",
    "At iteration: 1\n",
    "Cell: 0 value: -1\n",
    "Cell: 1 value: -1\n",
    "Cell: 2 value: -1\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -1\n",
    "Cell: 5 value: -1\n",
    "Cell: 6 value: -1\n",
    "Cell: 7 value: -1\n",
    "Cell: 8 value: -1\n",
    "Cell: 9 value: -1\n",
    "Cell: 10 value: -1\n",
    "Cell: 11 value: -1\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -1\n",
    "Cell: 14 value: -1\n",
    "Cell: 15 value: -1\n",
    "At iteration: 2\n",
    "Cell: 0 value: -2\n",
    "Cell: 1 value: -2\n",
    "Cell: 2 value: -1.75\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -2\n",
    "Cell: 5 value: -2\n",
    "Cell: 6 value: -2\n",
    "Cell: 7 value: -1.75\n",
    "Cell: 8 value: -1.75\n",
    "Cell: 9 value: -2\n",
    "Cell: 10 value: -2\n",
    "Cell: 11 value: -2\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -1.75\n",
    "Cell: 14 value: -2\n",
    "Cell: 15 value: -2\n",
    "At iteration: 3\n",
    "Cell: 0 value: -3\n",
    "Cell: 1 value: -2.9375\n",
    "Cell: 2 value: -2.4375\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -2.9375\n",
    "Cell: 5 value: -3\n",
    "Cell: 6 value: -2.875\n",
    "Cell: 7 value: -2.4375\n",
    "Cell: 8 value: -2.4375\n",
    "Cell: 9 value: -2.875\n",
    "Cell: 10 value: -3\n",
    "Cell: 11 value: -2.9375\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -2.4375\n",
    "Cell: 14 value: -2.9375\n",
    "Cell: 15 value: -3\n",
    "\n",
    "...\n",
    "\n",
    "At iteration: 130\n",
    "Cell: 0 value: -21.9815\n",
    "Cell: 1 value: -19.9835\n",
    "Cell: 2 value: -13.9889\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -19.9835\n",
    "Cell: 5 value: -19.9836\n",
    "Cell: 6 value: -17.9855\n",
    "Cell: 7 value: -13.9889\n",
    "Cell: 8 value: -13.9889\n",
    "Cell: 9 value: -17.9855\n",
    "Cell: 10 value: -19.9836\n",
    "Cell: 11 value: -19.9835\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -13.9889\n",
    "Cell: 14 value: -19.9835\n",
    "Cell: 15 value: -21.9815\n",
    "At iteration: 131\n",
    "Cell: 0 value: -21.9825\n",
    "Cell: 1 value: -19.9844\n",
    "Cell: 2 value: -13.9895\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -19.9844\n",
    "Cell: 5 value: -19.9845\n",
    "Cell: 6 value: -17.9862\n",
    "Cell: 7 value: -13.9895\n",
    "Cell: 8 value: -13.9895\n",
    "Cell: 9 value: -17.9862\n",
    "Cell: 10 value: -19.9845\n",
    "Cell: 11 value: -19.9844\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -13.9895\n",
    "Cell: 14 value: -19.9844\n",
    "Cell: 15 value: -21.9825\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exe.cpp</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
