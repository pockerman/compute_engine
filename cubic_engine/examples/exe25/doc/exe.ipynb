{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 25: Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Acknowledgements](#ackw)\n",
    "* [Overview](#overview) \n",
    "    * [Iterative policy evaluation](#ekf)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we go over yet another classical reinforcement learning algorithm; namely value iteration. This is a method for computing an optimal MDP poliy $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Iterative policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is actually a dynamic programming algorithm. With these algorithms, one should supply the model of the environment. As such, one can argue that classical DP algorithms are probably of limited utility in the field of reinforcement learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since here we will present a tabular based implementation, we will have to assume that the state, $S$, action $A$ and reward, $R$, sets are finite. Furthermore, we will assume that the dynamics can be described by the set of probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(s^{'}, r | s, \\alpha), ~~ \\forall s  \\in S, \\alpha \\in A, r \\in R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily obtain optimal policies once we have found the optimal functions that either $V^{*}$ or $Q^{*}$. These will satisfy respectively the following Bellman equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V^{*}(s) = max_{\\alpha} E[R_{t+1} + \\gamma V^{*}(s_{t+1}) | S_t = s, A_t = \\alpha]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q^{*}(s,\\alpha)= E[R_{t+1} + \\gamma max_{\\alpha^{'}}Q^{*}(s_{t+1}, \\alpha^{'}) | S_t = s, A_t = \\alpha]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to compute the state value function $V_{\\pi}$ for an arbitrary policy $\\pi$.  This is called policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will simulate an episodic MDP. The world is shown in the figure below. The gray boxes represent the goal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every transition we will assume a reward of $R=-1$. We also use $\\gamma=1$. Each state can have four possible actions; ```UP```, ```DOWN```, ```LEFT``` and ```RIGHT```. The code below uses the ```cengine::rl::worlds::GridWorldAction``` enumeration to describe that. If the agent goes off the world it is assumed to come back  on the  state that led it outside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"include_files\"></a> Include files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include \"cubic_engine/base/cubic_engine_types.h\"\n",
    "#include \"kernel/base/kernel_consts.h\"\n",
    "#include \"cubic_engine/rl/worlds/grid_world.h\"\n",
    "#include \"cubic_engine/rl/worlds/grid_world_action_space.h\"\n",
    "#include \"cubic_engine/rl/synchronous_value_function_learning.h\"\n",
    "#include \"cubic_engine/rl/reward_table.h\"\n",
    "\n",
    "#include <iostream>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "namespace example\n",
    "{\n",
    "\n",
    "using cengine::uint_t;\n",
    "using cengine::real_t;\n",
    "using cengine::rl::worlds::GridWorld;\n",
    "using cengine::rl::worlds::GridWorldAction;\n",
    "using cengine::rl::SyncValueFuncItr;\n",
    "using cengine::rl::SyncValueFuncItrInput;\n",
    "using cengine::rl::RewardTable;\n",
    "using kernel::CSVWriter;\n",
    "\n",
    "class RewardProducer\n",
    "{\n",
    "public:\n",
    "\n",
    "    typedef real_t value_t;\n",
    "\n",
    "    /// construcotr\n",
    "    RewardProducer();\n",
    "\n",
    "    /// returns the reward for the goal\n",
    "    real_t goal_reward()const{return 0.0;}\n",
    "\n",
    "    /// returns the reward for the action\n",
    "    /// at  state s when going to state sprime\n",
    "    template<typename ActionTp, typename StateTp>\n",
    "    real_t get_reward(const ActionTp& action,\n",
    "                      const StateTp& s,\n",
    "                      const StateTp& sprime)const{\n",
    "        return rewards_.get_reward(s.get_id(), action);\n",
    "    }\n",
    "\n",
    "    /// returns the reward for the action\n",
    "    /// at  state s when going to state sprime\n",
    "     template<typename ActionTp, typename StateTp>\n",
    "     real_t get_reward(const ActionTp& action,\n",
    "                          const StateTp& s)const{\n",
    "            return rewards_.get_reward(s.get_id(), action);\n",
    "     }\n",
    "\n",
    "private:\n",
    "\n",
    "    /// table that holds the rewards\n",
    "    RewardTable<GridWorldAction, real_t> rewards_;\n",
    "\n",
    "    /// setup the rewards\n",
    "    void setup_rewards();\n",
    "};\n",
    "\n",
    "RewardProducer::RewardProducer()\n",
    "    :\n",
    "   rewards_()\n",
    "{\n",
    "    setup_rewards();\n",
    "}\n",
    "\n",
    "void\n",
    "RewardProducer::setup_rewards(){\n",
    "\n",
    "    rewards_.set_reward(0, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(0, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(0, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(0, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(1, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(1, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(1, GridWorldAction::WEST, -1.0);\n",
    "    rewards_.set_reward(1, GridWorldAction::SOUTH, -1.0);\n",
    "\n",
    "    rewards_.set_reward(2, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(2, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(2, GridWorldAction::WEST, -1.0);\n",
    "    rewards_.set_reward(2, GridWorldAction::SOUTH, -1.0);\n",
    "\n",
    "    rewards_.set_reward(4, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(4, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(4, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(4, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(5, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(5, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(5, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(5, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(6, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(6, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(6, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(6, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(7, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(7, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(7, GridWorldAction::WEST, -1.0);\n",
    "    rewards_.set_reward(7, GridWorldAction::EAST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(8, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(8, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(8, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(8, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(9, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(9, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(9, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(9, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(10, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(10, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(10, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(10, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(11, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(11, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(11, GridWorldAction::WEST, -1.0);\n",
    "    rewards_.set_reward(11, GridWorldAction::EAST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(13, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(13, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(13, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(13, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(14, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(14, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(14, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(14, GridWorldAction::WEST, -1.0);\n",
    "\n",
    "    rewards_.set_reward(15, GridWorldAction::NORTH, -1.0);\n",
    "    rewards_.set_reward(15, GridWorldAction::EAST, -1.0);\n",
    "    rewards_.set_reward(15, GridWorldAction::SOUTH, -1.0);\n",
    "    rewards_.set_reward(15, GridWorldAction::WEST, -1.0);\n",
    "}\n",
    "\n",
    "typedef GridWorld<RewardProducer> world_t;\n",
    "typedef world_t::state_t state_t;\n",
    "\n",
    "const uint_t N_CELLS = 4;\n",
    "\n",
    "void\n",
    "create_wolrd(world_t& w){\n",
    "\n",
    "   std::vector<state_t> world_states;\n",
    "   world_states.reserve(N_CELLS*N_CELLS);\n",
    "\n",
    "   uint_t counter=0;\n",
    "   for(uint_t i=0; i<N_CELLS; ++i){\n",
    "       for(uint_t j=0; j<N_CELLS; ++j){\n",
    "           world_states.push_back(state_t(counter++));\n",
    "       }\n",
    "   }\n",
    "\n",
    "   w.set_states(std::move(world_states));\n",
    "\n",
    "   counter=0;\n",
    "   for(uint_t i=0; i<N_CELLS*N_CELLS; ++i){\n",
    "\n",
    "       auto& state = w.get_state(i);\n",
    "\n",
    "       /// bottom row\n",
    "       if(i <4){\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), &state);\n",
    "\n",
    "           if(i != 3){\n",
    "             state.set_transition(GridWorldAction::EAST, &w.get_state(i+1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(GridWorldAction::EAST, &state);\n",
    "           }\n",
    "\n",
    "           state.set_transition(GridWorldAction::NORTH, &w.get_state(N_CELLS + i));\n",
    "\n",
    "           if(i == 0){\n",
    "                state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &state);\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "       }\n",
    "       else if(i >= 12 ){\n",
    "           /// top row\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), &w.get_state(i - N_CELLS));\n",
    "\n",
    "           if(i != 15){\n",
    "             state.set_transition(GridWorldAction::EAST, &w.get_state(i+1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(GridWorldAction::EAST, &state);\n",
    "           }\n",
    "\n",
    "           state.set_transition(GridWorldAction::NORTH, &state);\n",
    "\n",
    "           if(i == 12){\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &state);\n",
    "           }\n",
    "           else{\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "       }\n",
    "       else{\n",
    "\n",
    "           /// all rows in between\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), &w.get_state(i - N_CELLS));\n",
    "\n",
    "           if(i != 11 && i != 7){\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::EAST), &w.get_state(i +1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::EAST), &state);\n",
    "           }\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::NORTH), &w.get_state(i + N_CELLS));\n",
    "\n",
    "           if(i != 4 && i != 8 ){\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "           else {\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &state);\n",
    "           }\n",
    "       }\n",
    "   }\n",
    "}\n",
    "\n",
    "}\n",
    "\n",
    "int main(){\n",
    "\n",
    "    using namespace example;\n",
    "\n",
    "    try{\n",
    "\n",
    "        typedef GridWorld<RewardProducer> world_t;\n",
    "        typedef world_t::state_t state_t;\n",
    "        typedef world_t::action_t action_t;\n",
    "\n",
    "        auto policy = [](const action_t&, const state_t&){\n",
    "          return 0.25;\n",
    "        };\n",
    "\n",
    "        RewardProducer rproducer;\n",
    "        auto dynamics = [&rproducer](const state_t& s1, real_t,\n",
    "                const state_t& s2, const action_t& action){\n",
    "          return 0.25;\n",
    "        };\n",
    "\n",
    "        std::vector<real_t> rewards(1, -1.0);\n",
    "\n",
    "        /// the world of the agent\n",
    "        world_t world;\n",
    "        create_wolrd(world);\n",
    "\n",
    "\n",
    "        std::cout<<\"Number of states: \"<<world.n_states()<<std::endl;\n",
    "\n",
    "        state_t start(15);\n",
    "        state_t goal1(3);\n",
    "        state_t goal2(12);\n",
    "\n",
    "        world.append_goal(goal1);\n",
    "        world.append_goal(goal2);\n",
    "\n",
    "        /// simulation parameters\n",
    "        /// number of episodes for the agent to learn.\n",
    "        const uint_t N_ITERATIONS = 160;\n",
    "        const real_t TOL = 0.001;\n",
    "        const real_t GAMMA = 1.0;\n",
    "\n",
    "        SyncValueFuncItrInput input={TOL, GAMMA, N_ITERATIONS, true};\n",
    "        SyncValueFuncItr<world_t> learner(std::move(input));\n",
    "\n",
    "        std::vector<real_t> row(2);\n",
    "        learner.initialize(world, 0.0);\n",
    "\n",
    "        world.restart(start);\n",
    "\n",
    "        while(learner.continue_iterations()){\n",
    "\n",
    "            std::cout<<\"At iteration: \"<<learner.get_current_iteration()<<std::endl;\n",
    "\n",
    "            learner.step(policy, dynamics);\n",
    "            auto values = learner.get_values();\n",
    "\n",
    "            for(auto c=0; c<values.size(); ++c){\n",
    "                std::cout<<\"Cell: \"<<c<<\" value: \"<<values[c]<<std::endl;\n",
    "            }   \n",
    "        }    \n",
    "    }\n",
    "    catch(std::exception& e){\n",
    "\n",
    "        std::cerr<<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(...){\n",
    "\n",
    "        std::cerr<<\"Unknown exception occured\"<<std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Number of states: 16\n",
    "At iteration: 1\n",
    "Cell: 0 value: -1\n",
    "Cell: 1 value: -1\n",
    "Cell: 2 value: -1\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -1\n",
    "Cell: 5 value: -1\n",
    "Cell: 6 value: -1\n",
    "Cell: 7 value: -1\n",
    "Cell: 8 value: -1\n",
    "Cell: 9 value: -1\n",
    "Cell: 10 value: -1\n",
    "Cell: 11 value: -1\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -1\n",
    "Cell: 14 value: -1\n",
    "Cell: 15 value: -1\n",
    "At iteration: 2\n",
    "Cell: 0 value: -2\n",
    "Cell: 1 value: -2\n",
    "Cell: 2 value: -1.75\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -2\n",
    "Cell: 5 value: -2\n",
    "Cell: 6 value: -2\n",
    "Cell: 7 value: -1.75\n",
    "Cell: 8 value: -1.75\n",
    "Cell: 9 value: -2\n",
    "Cell: 10 value: -2\n",
    "Cell: 11 value: -2\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -1.75\n",
    "Cell: 14 value: -2\n",
    "Cell: 15 value: -2\n",
    "At iteration: 3\n",
    "Cell: 0 value: -3\n",
    "Cell: 1 value: -2.9375\n",
    "Cell: 2 value: -2.4375\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -2.9375\n",
    "Cell: 5 value: -3\n",
    "Cell: 6 value: -2.875\n",
    "Cell: 7 value: -2.4375\n",
    "Cell: 8 value: -2.4375\n",
    "Cell: 9 value: -2.875\n",
    "Cell: 10 value: -3\n",
    "Cell: 11 value: -2.9375\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -2.4375\n",
    "Cell: 14 value: -2.9375\n",
    "Cell: 15 value: -3\n",
    "\n",
    "...\n",
    "\n",
    "At iteration: 130\n",
    "Cell: 0 value: -21.9815\n",
    "Cell: 1 value: -19.9835\n",
    "Cell: 2 value: -13.9889\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -19.9835\n",
    "Cell: 5 value: -19.9836\n",
    "Cell: 6 value: -17.9855\n",
    "Cell: 7 value: -13.9889\n",
    "Cell: 8 value: -13.9889\n",
    "Cell: 9 value: -17.9855\n",
    "Cell: 10 value: -19.9836\n",
    "Cell: 11 value: -19.9835\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -13.9889\n",
    "Cell: 14 value: -19.9835\n",
    "Cell: 15 value: -21.9815\n",
    "At iteration: 131\n",
    "Cell: 0 value: -21.9825\n",
    "Cell: 1 value: -19.9844\n",
    "Cell: 2 value: -13.9895\n",
    "Cell: 3 value: 0\n",
    "Cell: 4 value: -19.9844\n",
    "Cell: 5 value: -19.9845\n",
    "Cell: 6 value: -17.9862\n",
    "Cell: 7 value: -13.9895\n",
    "Cell: 8 value: -13.9895\n",
    "Cell: 9 value: -17.9862\n",
    "Cell: 10 value: -19.9845\n",
    "Cell: 11 value: -19.9844\n",
    "Cell: 12 value: 0\n",
    "Cell: 13 value: -13.9895\n",
    "Cell: 14 value: -19.9844\n",
    "Cell: 15 value: -21.9825\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exe.cpp</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
