{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 6:  Q-learning applied on simple ```GridWorld```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Acknowledgements](#ackw)\n",
    "* [Overview](#overview) \n",
    "    * [Q-learning](#ekf)\n",
    "    * [Test case](#test_case)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ackw\"></a> Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the content in this notebook is taken from the book ```Reinforcement Learning An introduction``` by Sutton and Barto and from the wikipedia <a href=\"https://en.wikipedia.org/wiki/Q-learning\">Q-learning</a> entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook discusses <a href=\"https://en.wikipedia.org/wiki/Q-learning\">Q-learning</a>. Q-learning is a direct method meaning that it tries to approximate the optimal action-value function $q_{*}$ directly. The algorithm keeps an estimate $Q(s, \\alpha)$ of $q_{*}(s, \\alpha)$ for each state-action pair $(s,\\alpha) \\in S \\times A$. Q-learning is an instance of TD algorithms since the updates are based (see the update equation below) on the so-called TD error $\\delta$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\delta_{t+1} =  R_{t+1} + \\gamma max_{\\alpha}Q(s_{t+1}, \\alpha) - Q(s_t, \\alpha_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, Q-learning is a model-free RL algorithm for learning  a policy by telling an agent what action to take under a given state $s$ or, more generally, under given circumstances . It does not require a model of the environment; hence the connotation model-free. Furthermore, the algorithm  can handle problems with stochastic transitions and rewards, without requiring adaptations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any finite Markov decision process (FMDP), Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. The  update equation for the state-action function is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(s_t, \\alpha_t) = Q(s_t, \\alpha_t) + \\eta[R_{t+1} + \\gamma max_{\\alpha}Q(s_{t+1}, \\alpha) - Q(s_t, \\alpha_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or more compactly by using the TD error $\\delta$ defined above as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(s_t, \\alpha_t) = Q(s_t, \\alpha_t) + \\eta \\delta_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\eta \\in (0,1]$ is  a learning rate and $\\gamma$ is the discount factor. In contrast to the iterative policy evaluation algorithm, the learned action-value function $Q$ approximates $q_{*}$ independent of the policy that is followed. This is why Q-learning is classified as an off-policy method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is shown below (see also ```Reinforcement Learning An introduction``` by Sutton and Barto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Inputs step size eta, small epsilon >0, gamma\n",
    "Initialize Q(s, a) for all s in S and a in A arbitrarily excpet from Q(goal, )  = 0\n",
    "for each episode:\n",
    "  initialize S \n",
    "  for eaach step of episode:\n",
    "      choose A from S using policy derived from Q (e.g. epsilon-greedy)\n",
    "      Take action A and observe R and S_prime\n",
    "      Q(S, A) = Q(S, A) + eta[R + gamma max_{a}Q(S_prime, A)- Q(S,A)]\n",
    "      S = S_prime\n",
    "   until S is goal\n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"test_case\"></a> Test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test case we will consider a $7\\times 7$ grid. The outer cells marked with $O$ and the inner cells marked with $I$. There are two possible feasible paths as shown in the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following reward scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount factor $\\gamma = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class ```QTableLearning``` as the name suggests implements a table based verion on the Q-learning algorithm. Thus, the actions, states and rewards sets are assumed finite.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "namespace exe\n",
    "{\n",
    "\n",
    "using cengine::uint_t;\n",
    "using cengine::real_t;\n",
    "using cengine::rl::worlds::GridWorld;\n",
    "using cengine::rl::worlds::GridWorldAction;\n",
    "using cengine::rl::QTableLearning;\n",
    "using cengine::rl::QLearningInput;\n",
    "using cengine::rl::RewardTable;\n",
    "using kernel::CSVWriter;\n",
    "const uint_t N_CELLS = 7;\n",
    "const real_t PENALTY = -100;\n",
    "\n",
    "\n",
    "class RewardProducer\n",
    "{\n",
    "public:\n",
    "\n",
    "    typedef real_t value_t;\n",
    "\n",
    "    /// construcotr\n",
    "    RewardProducer();\n",
    "\n",
    "    /// returns the reward for the goal\n",
    "    real_t goal_reward()const{return 0.0;}\n",
    "\n",
    "    /// returns the reward for the action\n",
    "    /// at  state s when going to state sprime\n",
    "    template<typename ActionTp, typename StateTp>\n",
    "    real_t get_reward(const ActionTp& action,\n",
    "                      const StateTp& s,\n",
    "                      const StateTp& sprime)const;\n",
    "\n",
    "private:\n",
    "\n",
    "    /// table that holds the rewards\n",
    "    RewardTable<GridWorldAction, real_t> rewards_;\n",
    "\n",
    "    /// setup the rewards\n",
    "    void setup_rewards();\n",
    "};\n",
    "\n",
    "RewardProducer::RewardProducer()\n",
    "    :\n",
    "      rewards_()\n",
    "{\n",
    "    setup_rewards();\n",
    "}\n",
    "\n",
    "void\n",
    "RewardProducer::setup_rewards(){\n",
    "\n",
    "    for(uint_t i=0; i<N_CELLS*N_CELLS; ++i){\n",
    "\n",
    "        if(i<7){\n",
    "\n",
    "            if(i != 6 && i !=0 ){\n",
    "                rewards_.add_reward(i, GridWorldAction::EAST,  PENALTY);\n",
    "                rewards_.add_reward(i, GridWorldAction::NORTH,  -1.0);\n",
    "                rewards_.add_reward(i, GridWorldAction::WEST,  PENALTY);\n",
    "            }\n",
    "            else{\n",
    "\n",
    "               if(i == 0){\n",
    "                   rewards_.add_reward(i, GridWorldAction::EAST,  PENALTY);\n",
    "               }\n",
    "               else{\n",
    "                   rewards_.add_reward(i, GridWorldAction::WEST,  PENALTY);\n",
    "               }\n",
    "\n",
    "               rewards_.add_reward(i, GridWorldAction::NORTH,  PENALTY);\n",
    "            }\n",
    "        }\n",
    "        else if(i>=42){\n",
    "\n",
    "            if(i != 42 && i != 48 ){\n",
    "                rewards_.add_reward(i, GridWorldAction::EAST,  PENALTY);\n",
    "\n",
    "                if(i != 43){\n",
    "                    rewards_.add_reward(i, GridWorldAction::SOUTH,  -1.0);\n",
    "                }\n",
    "                else{\n",
    "                   rewards_.add_reward(i, GridWorldAction::SOUTH,  0.0);\n",
    "                }\n",
    "\n",
    "                rewards_.add_reward(i, GridWorldAction::WEST,  PENALTY);\n",
    "            }\n",
    "            else{\n",
    "               rewards_.add_reward(i, GridWorldAction::SOUTH,  PENALTY);\n",
    "\n",
    "               if( i == 42){\n",
    "                    rewards_.add_reward(i, GridWorldAction::EAST,  PENALTY);\n",
    "               }\n",
    "\n",
    "               if(i == 48){\n",
    "                   rewards_.add_reward(i, GridWorldAction::WEST,  PENALTY);\n",
    "               }\n",
    "            }\n",
    "\n",
    "\n",
    "        }\n",
    "        else{\n",
    "\n",
    "            static const uint_t arwest[]={7,14,21,28, 35};\n",
    "            static const uint_t areast[]={13,20,27,34, 41};\n",
    "\n",
    "            if(std::find(&arwest[0],\n",
    "                         &arwest[ sizeof(arwest)/sizeof(uint_t) ],\n",
    "                         i) != &arwest[ sizeof (arwest)/sizeof(uint_t) ]){\n",
    "\n",
    "                rewards_.add_reward(i, GridWorldAction::SOUTH,  PENALTY);\n",
    "                rewards_.add_reward(i, GridWorldAction::NORTH,  PENALTY);\n",
    "\n",
    "                if(i != 35){\n",
    "                    rewards_.add_reward(i, GridWorldAction::EAST,  -1.0);\n",
    "                }\n",
    "                else{\n",
    "                   rewards_.add_reward(i, GridWorldAction::EAST,  0.0);\n",
    "                }\n",
    "            }\n",
    "            else if(std::find(&areast[0],\n",
    "                              &areast[ sizeof(areast)/sizeof(uint_t) ],\n",
    "                              i) != &areast[ sizeof (areast)/sizeof(uint_t) ]){\n",
    "\n",
    "                rewards_.add_reward(i, GridWorldAction::SOUTH,  PENALTY);\n",
    "                rewards_.add_reward(i, GridWorldAction::NORTH,  PENALTY);\n",
    "\n",
    "                if(i != 13){\n",
    "                    rewards_.add_reward(i, GridWorldAction::WEST,  -1.0);\n",
    "                }\n",
    "                else{\n",
    "                   rewards_.add_reward(i, GridWorldAction::WEST,  0.0);\n",
    "                }\n",
    "            }\n",
    "            else{\n",
    "\n",
    "                static uint_t short_path_1[] ={8, 15, 22, 29, 36};\n",
    "                static uint_t short_path_2[] ={37, 38, 39, 40};\n",
    "                static uint_t short_path_3[] ={33, 26, 13, 12};\n",
    "                static uint_t short_path_4[] ={9, 10, 11};\n",
    "\n",
    "                if(std::find(&short_path_1[0],\n",
    "                             &short_path_1[ sizeof(short_path_1)/sizeof(uint_t) ],\n",
    "                                         i) != &short_path_1[ sizeof (short_path_1)/sizeof(uint_t) ]){\n",
    "\n",
    "\n",
    "                     if(i != 36){\n",
    "                         rewards_.add_reward(i, GridWorldAction::NORTH,  -2.0);\n",
    "                         rewards_.add_reward(i, GridWorldAction::WEST,  PENALTY);\n",
    "\n",
    "                         if(i != 8){\n",
    "                             rewards_.add_reward(i, GridWorldAction::EAST,  -3.);\n",
    "                             rewards_.add_reward(i, GridWorldAction::SOUTH,  -1.0);\n",
    "                         }\n",
    "                         else{\n",
    "                             rewards_.add_reward(i, GridWorldAction::EAST,  -1.0);\n",
    "                             rewards_.add_reward(i, GridWorldAction::SOUTH,  PENALTY);\n",
    "                         }\n",
    "                     }\n",
    "                     else{\n",
    "                        rewards_.add_reward(i, GridWorldAction::NORTH,  PENALTY);\n",
    "                        rewards_.add_reward(i, GridWorldAction::WEST,  PENALTY);\n",
    "                        rewards_.add_reward(i, GridWorldAction::EAST,  -1.0);\n",
    "                        rewards_.add_reward(i, GridWorldAction::SOUTH,  -1.0);\n",
    "                     }\n",
    "                }\n",
    "                else if(std::find(&short_path_2[0],\n",
    "                                  &short_path_2[ sizeof(short_path_2)/sizeof(uint_t) ],\n",
    "                                              i) != &short_path_2[ sizeof (short_path_2)/sizeof(uint_t) ]){\n",
    "\n",
    "\n",
    "                    rewards_.add_reward(i, GridWorldAction::NORTH,  PENALTY);\n",
    "\n",
    "                    if(i != 40){\n",
    "                        rewards_.add_reward(i, GridWorldAction::SOUTH,  -3.0);\n",
    "                        rewards_.add_reward(i, GridWorldAction::EAST,  -1.0);\n",
    "                        rewards_.add_reward(i, GridWorldAction::WEST,  -2.0);\n",
    "                    }\n",
    "                    else{\n",
    "                        rewards_.add_reward(i, GridWorldAction::SOUTH,  -1.0);\n",
    "                        rewards_.add_reward(i, GridWorldAction::EAST,  PENALTY);\n",
    "                        rewards_.add_reward(i, GridWorldAction::WEST,  -2.0);\n",
    "                    }\n",
    "\n",
    "                }\n",
    "                else if(std::find(&short_path_3[0],\n",
    "                                  &short_path_3[ sizeof(short_path_3)/sizeof(uint_t) ],\n",
    "                                              i) != &short_path_3[ sizeof (short_path_3)/sizeof(uint_t) ]){\n",
    "\n",
    "\n",
    "                    rewards_.add_reward(i, GridWorldAction::NORTH,  -2.0);\n",
    "                    rewards_.add_reward(i, GridWorldAction::EAST,  PENALTY);\n",
    "\n",
    "                    if(i != 12){\n",
    "                        rewards_.add_reward(i, GridWorldAction::SOUTH,  -1.0);\n",
    "                        rewards_.add_reward(i, GridWorldAction::WEST,  -3.0);\n",
    "                    }\n",
    "                    else{\n",
    "                        rewards_.add_reward(i, GridWorldAction::SOUTH,  PENALTY);\n",
    "                        rewards_.add_reward(i, GridWorldAction::WEST,  -2.0);\n",
    "                    }\n",
    "                }\n",
    "                else if(std::find(&short_path_4[0],\n",
    "                                  &short_path_4[ sizeof(short_path_4)/sizeof(uint_t) ],\n",
    "                                              i) != &short_path_4[ sizeof (short_path_4)/sizeof(uint_t) ]){\n",
    "\n",
    "                    rewards_.add_reward(i, GridWorldAction::NORTH,  -3.);\n",
    "                    rewards_.add_reward(i, GridWorldAction::SOUTH,  PENALTY);\n",
    "                    rewards_.add_reward(i, GridWorldAction::WEST,  -2.0);\n",
    "\n",
    "                    if(i==11){\n",
    "                       rewards_.add_reward(i, GridWorldAction::EAST,  0.);\n",
    "                    }\n",
    "                    else{\n",
    "                        rewards_.add_reward(i, GridWorldAction::EAST,  -1.);\n",
    "                    }\n",
    "                }\n",
    "                else{\n",
    "\n",
    "                    if(i == 30 || i == 31 || i == 32){\n",
    "\n",
    "                        rewards_.add_reward(i, GridWorldAction::NORTH,  -1.0);\n",
    "                        rewards_.add_reward(i, GridWorldAction::SOUTH,  -3.0);\n",
    "\n",
    "                        if(i == 32){\n",
    "                            rewards_.add_reward(i, GridWorldAction::EAST,  -1.0);\n",
    "                        }\n",
    "                        else{\n",
    "                            rewards_.add_reward(i, GridWorldAction::EAST,  -3.0);\n",
    "                        }\n",
    "\n",
    "                        if(i == 30){\n",
    "                            rewards_.add_reward(i, GridWorldAction::WEST,  -1.0);\n",
    "                        }\n",
    "                        else{\n",
    "                            rewards_.add_reward(i, GridWorldAction::WEST,  -3.0);\n",
    "                        }\n",
    "                    }\n",
    "                    else if (i == 23 || i == 24 || i == 25) {\n",
    "\n",
    "                        rewards_.add_reward(i, GridWorldAction::NORTH,  -3.0);\n",
    "                        rewards_.add_reward(i, GridWorldAction::SOUTH,  -3.0);\n",
    "\n",
    "                        if(i == 25){\n",
    "                            rewards_.add_reward(i, GridWorldAction::EAST,  -1.0);\n",
    "                        }\n",
    "                        else{\n",
    "                            rewards_.add_reward(i, GridWorldAction::EAST,  -3.0);\n",
    "                        }\n",
    "\n",
    "                        if(i == 23){\n",
    "                            rewards_.add_reward(i, GridWorldAction::WEST,  -1.0);\n",
    "                        }\n",
    "                        else{\n",
    "                            rewards_.add_reward(i, GridWorldAction::WEST,  -3.0);\n",
    "                        }\n",
    "                    }\n",
    "                    else{\n",
    "                        rewards_.add_reward(i, GridWorldAction::NORTH,  -3.0);\n",
    "                        rewards_.add_reward(i, GridWorldAction::SOUTH,  -1.0);\n",
    "\n",
    "                        if(i == 18){\n",
    "                            rewards_.add_reward(i, GridWorldAction::EAST,  -1.0);\n",
    "                        }\n",
    "                        else{\n",
    "                            rewards_.add_reward(i, GridWorldAction::EAST,  -3.0);\n",
    "                        }\n",
    "\n",
    "                        if(i == 16){\n",
    "                            rewards_.add_reward(i, GridWorldAction::WEST,  -1.0);\n",
    "                        }\n",
    "                        else{\n",
    "                            rewards_.add_reward(i, GridWorldAction::WEST,  -3.0);\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "template<typename ActionTp, typename StateTp>\n",
    "real_t\n",
    "RewardProducer::get_reward(const ActionTp& action,\n",
    "                           const StateTp& s,\n",
    "                           const StateTp& sprime)const{\n",
    "\n",
    "    return rewards_.get_reward(s.get_id(), action);\n",
    "\n",
    "}\n",
    "\n",
    "typedef GridWorld<RewardProducer> world_t;\n",
    "typedef world_t::state_t state_t;\n",
    "\n",
    "void\n",
    "create_wolrd(world_t& w){\n",
    "\n",
    "   std::vector<state_t> world_states;\n",
    "   world_states.reserve(N_CELLS*N_CELLS);\n",
    "\n",
    "   uint_t counter=0;\n",
    "   for(uint_t i=0; i<N_CELLS; ++i){\n",
    "       for(uint_t j=0; j<N_CELLS; ++j){\n",
    "           world_states.push_back(state_t(counter++));\n",
    "       }\n",
    "   }\n",
    "\n",
    "   w.set_states(std::move(world_states));\n",
    "\n",
    "   counter=0;\n",
    "   for(uint_t i=0; i<N_CELLS*N_CELLS; ++i){\n",
    "\n",
    "       auto& state = w.get_state(i);\n",
    "\n",
    "       /// bottom row\n",
    "       if(i <7){\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), nullptr);\n",
    "\n",
    "           if(i != 6){\n",
    "             state.set_transition(GridWorldAction::EAST, &w.get_state(i+1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(GridWorldAction::EAST, nullptr);\n",
    "           }\n",
    "\n",
    "           state.set_transition(GridWorldAction::NORTH, &w.get_state(N_CELLS + i));\n",
    "\n",
    "           if(i == 0){\n",
    "                state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), nullptr);\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "       }\n",
    "       else if(i >= 42 ){\n",
    "           /// top row\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), &w.get_state(i - N_CELLS));\n",
    "\n",
    "           if(i != 48){\n",
    "             state.set_transition(GridWorldAction::EAST, &w.get_state(i+1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(GridWorldAction::EAST, nullptr);\n",
    "           }\n",
    "\n",
    "           state.set_transition(GridWorldAction::NORTH, nullptr);\n",
    "\n",
    "           if(i == 42){\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), nullptr);\n",
    "           }\n",
    "           else{\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "       }\n",
    "       else{\n",
    "\n",
    "           /// all rows in between\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::SOUTH), &w.get_state(i - N_CELLS));\n",
    "\n",
    "           if(i != 13 && i != 20 && i != 27 && i != 41 && i != 34){\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::EAST), &w.get_state(i +1));\n",
    "           }\n",
    "           else{\n",
    "               state.set_transition(static_cast<GridWorldAction>(GridWorldAction::EAST), nullptr);\n",
    "           }\n",
    "\n",
    "           state.set_transition(static_cast<GridWorldAction>(GridWorldAction::NORTH), &w.get_state(i + N_CELLS));\n",
    "\n",
    "           if(i != 7 && i != 14 && i != 21 && i != 28 && i != 35){\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), &w.get_state(i-1));\n",
    "           }\n",
    "           else {\n",
    "              state.set_transition(static_cast<GridWorldAction>(GridWorldAction::WEST), nullptr);\n",
    "           }\n",
    "       }\n",
    "   }\n",
    "}\n",
    "\n",
    "}\n",
    "\n",
    "int main() {\n",
    "\n",
    "    using namespace exe;\n",
    "\n",
    "    /// the world of the agent\n",
    "    world_t world;\n",
    "\n",
    "    create_wolrd(world);\n",
    "\n",
    "    std::cout<<\"Number of states: \"<<world.n_states()<<std::endl;\n",
    "\n",
    "    state_t start(36);\n",
    "    state_t goal(12);\n",
    "\n",
    "    /// simulation parameters\n",
    "    const real_t EPSILON = 0.1;\n",
    "\n",
    "    QLearningInput qinput={1.0, EPSILON, 0.0, true, true};\n",
    "    QTableLearning<world_t> qlearner(std::move(qinput));\n",
    "\n",
    "    CSVWriter writer(\"agent_rewards.csv\", ',', true);\n",
    "\n",
    "    writer.write_column_names({\"Episode\", \"Reward\"}, true);\n",
    "\n",
    "    /// number of episodes for the agent to\n",
    "    /// learn the Q-values.\n",
    "    const uint_t N_ITERATIONS = 10000;\n",
    "\n",
    "    std::vector<real_t> row(2);\n",
    "    qlearner.initialize(world, PENALTY);\n",
    "\n",
    "    for(uint_t episode=0; episode < N_ITERATIONS; ++episode){\n",
    "        world.restart(start, goal);\n",
    "        qlearner.train(goal);\n",
    "\n",
    "        auto reward = qlearner.get_table().get_total_reward();\n",
    "        writer.write_row(std::make_tuple(episode, reward));\n",
    "        std::cout<<\"At episode: \"<<episode<<\" total reward: \"<<reward<<std::endl;\n",
    "    }\n",
    "\n",
    "    /// now that we train let's play\n",
    "    auto& qtable = qlearner.get_table();\n",
    "\n",
    "    auto stop = false;\n",
    "\n",
    "    world.restart(start, goal);\n",
    "\n",
    "    auto counter = 0;\n",
    "    while(!stop){\n",
    "\n",
    "        auto& state = world.get_current_state();\n",
    "        std::cout<<\"At state: \"<<state.get_id()<<std::endl;\n",
    "\n",
    "        if(state == goal){\n",
    "            std::cout<<\"State: \"<<state.get_id()<<\" is the goal\"<<std::endl;\n",
    "            break;\n",
    "        }\n",
    "\n",
    "        /// get the action with the maximum value\n",
    "        auto action = qtable.get_max_reward_action_at_state(state.get_id());\n",
    "\n",
    "        std::cout<<\"Maximum reward action: \"<<cengine::rl::worlds::to_string(action)<<std::endl;\n",
    "        std::cout<<\"MAximum reward is: \"<<qtable.get_max_reward_at_state(state.get_id())<<std::endl;\n",
    "        world.execute_action(action);\n",
    "        counter++;\n",
    "    }\n",
    "    \n",
    "   return 0;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "...\n",
    "At episode: 9997 total reward: -7767\n",
    "At iteration: 1\n",
    "\tCurrent state: 36\n",
    "\t Setting for state: 36 and action: SOUTH to value: -1\n",
    "At iteration: 2\n",
    "\tCurrent state: 29\n",
    "\t Setting for state: 29 and action: SOUTH to value: -1\n",
    "At iteration: 3\n",
    "\tCurrent state: 22\n",
    "\t Setting for state: 22 and action: SOUTH to value: -1\n",
    "At iteration: 4\n",
    "\tCurrent state: 15\n",
    "\t Setting for state: 15 and action: NORTH to value: -2\n",
    "At iteration: 5\n",
    "\tCurrent state: 22\n",
    "\t Setting for state: 22 and action: SOUTH to value: -1\n",
    "At iteration: 6\n",
    "\tCurrent state: 15\n",
    "\t Setting for state: 15 and action: SOUTH to value: -1\n",
    "At iteration: 7\n",
    "\tCurrent state: 8\n",
    "\t Setting for state: 8 and action: EAST to value: -1\n",
    "At iteration: 8\n",
    "\tCurrent state: 9\n",
    "\t Setting for state: 9 and action: NORTH to value: -3\n",
    "At iteration: 9\n",
    "\tCurrent state: 16\n",
    "\t Setting for state: 16 and action: SOUTH to value: -1\n",
    "At iteration: 10\n",
    "\tCurrent state: 9\n",
    "\t Setting for state: 9 and action: EAST to value: -1\n",
    "At iteration: 11\n",
    "\tCurrent state: 10\n",
    "\t Setting for state: 10 and action: EAST to value: -1\n",
    "At iteration: 12\n",
    "\tCurrent state: 11\n",
    "\t Setting for state: 11 and action: EAST to value: 0\n",
    "At episode: 9998 total reward: -7767\n",
    "At iteration: 1\n",
    "\tCurrent state: 36\n",
    "\t Setting for state: 36 and action: SOUTH to value: -1\n",
    "At iteration: 2\n",
    "\tCurrent state: 29\n",
    "\t Setting for state: 29 and action: SOUTH to value: -1\n",
    "At iteration: 3\n",
    "\tCurrent state: 22\n",
    "\t Setting for state: 22 and action: SOUTH to value: -1\n",
    "At iteration: 4\n",
    "\tCurrent state: 15\n",
    "\t Setting for state: 15 and action: SOUTH to value: -1\n",
    "At iteration: 5\n",
    "\tCurrent state: 8\n",
    "\t Setting for state: 8 and action: EAST to value: -1\n",
    "At iteration: 6\n",
    "\tCurrent state: 9\n",
    "\t Setting for state: 9 and action: EAST to value: -1\n",
    "At iteration: 7\n",
    "\tCurrent state: 10\n",
    "\t Setting for state: 10 and action: EAST to value: -1\n",
    "At iteration: 8\n",
    "\tCurrent state: 11\n",
    "\t Setting for state: 11 and action: EAST to value: 0\n",
    "At episode: 9999 total reward: -7767\n",
    "At state: 36\n",
    "Maximum reward action: SOUTH\n",
    "MAximum reward is: -1\n",
    "At state: 29\n",
    "Maximum reward action: SOUTH\n",
    "MAximum reward is: -1\n",
    "At state: 22\n",
    "Maximum reward action: SOUTH\n",
    "MAximum reward is: -1\n",
    "At state: 15\n",
    "Maximum reward action: SOUTH\n",
    "MAximum reward is: -1\n",
    "At state: 8\n",
    "Maximum reward action: EAST\n",
    "MAximum reward is: -1\n",
    "At state: 9\n",
    "Maximum reward action: EAST\n",
    "MAximum reward is: -1\n",
    "At state: 10\n",
    "Maximum reward action: EAST\n",
    "MAximum reward is: -1\n",
    "At state: 11\n",
    "Maximum reward action: EAST\n",
    "MAximum reward is: 0\n",
    "At state: 12\n",
    "State: 12 is the goal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sum of rewards](sum_of_rewards.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exc.cpp</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
