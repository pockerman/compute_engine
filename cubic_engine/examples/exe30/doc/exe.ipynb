{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 30: PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Acknowledgements](#ackw)\n",
    "* [Overview](#overview) \n",
    "    * [Priiciapl Component Analysis](#ekf)\n",
    "    * [Summary PCA](#sumekf)\n",
    "    * [Test Case](#motion_model)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Principal Component Analysis</a> for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction refres to a number of techniques for reducing the dimensions associated with a data set. Consider, for example, a data set where each input point has five features. A dimensionality reduction technique can help us reduce the number of features to three or two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One perhaps aparent reason why one would like to reduce the number of features in a data set is visualization. It is easy to visualize two or even three dimensional data sets. However, as the dimensions increase, the difficulty of doing so also increases both in terms of computing power required as well as the conceptual understanding.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from visualization, one other reason why someone would like to reduce the number of dimensions of a data set is that a large number  of input features can cause poor performance of ML algorithms. This is frequently abbreviated as the curse of dimensionality. Furthermore, often fewer input dimensions translate to fewer model parameters or simpler model structure in general. A model with too many parameters is likely to overfit the training set and therefore it may not perform well on new data. Finally, dimensionality reduction may also be appropriate when the variables in a dataset are noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In generaly, there are two main classes of dimensionality reduction technique namely feature selection and feature extraction. With feature selection, as the name implies, we somehow select a subset of the original features. On the other hand, with feature extraction, we derive information from the data set to build or create new features altogether. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can be thought of as fitting a p-dimensional ellipsoid to the data (see figure below). Each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PCA eignevectors](pca_vectors.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it appears that the major question is how can we find the axes of the ellipsoid? It turns out that we can have structured approach towards this direction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We first center the data around the origin by subtracting the mean of each variable from the dataset. \n",
    "2. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix e.g. eigenvalue decomposition. \n",
    "3. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors from step,  are the eignevectors of the matrix $\\mathbf{X}^T\\mathbf{X}$. Thus they should satisfy the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{X}^T  \\mathbf{X}\\mathbf{w}_j = \\lambda_j\\mathbf{w}_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda_j$ is the eigenvalue corresponding to the eigenvector $\\mathbf{w}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the matrix $\\mathbf{X}^T  \\mathbf{X}$ is symmetric and therefore the eigenvectors are orthogonal. At step three we normalize $\\mathbf{w}_j$ to havel length one. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other approach to do PCA is by performing a <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\"> Singular Value Decomposition</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD of a matrix $\\mathbf{A}$ has the following form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{\\Sigma}$ is  a diagonal-like matrix containing the singular values $\\sigma_i$ and $\\mathbf{V}$ is an orthogonal matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schematically, SVD is shown in the figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVD Schematics](svd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues $\\lambda_i$ of $\\mathbf{A}^T\\mathbf{A}$ and the singular values $\\sigma_i$ of $\\mathbf{A}$ are connected via:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda_i = \\sigma_{i}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the columns of $\\mathbf{V}$ are the eigenvectors for $\\mathbf{A}^T\\mathbf{A}$. Moreover, the SVD process orders the singular values according to their size i.e.: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can use as $\\mathbf{w}_1$ the first column of $\\mathbf{V}$ as this will be the eigenvector for the largest singular value $\\sigma_1 = \\sqrt{\\lambda_1}$. Similarly for the rest $\\mathbf{w}_i$. The columns of $\\mathbf{V}$ are called the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very frequently when we discuss PCA, the terms total variance and variance explained frquently come up. Let's briefly see how these terms are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple words, the total variance is the sum of variances of all individual principal components. Moreover, the fraction of variance explained by a principal component is the ratio between the variance of that principal component and the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok but how do we calculate this? This is simple once we know the principal components or the eigenvectors of $\\mathbf{X}^T  \\mathbf{X}$. Concretely, each column of $\\mathbf{V}$ is such an eignevector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, if we compute the total variance over the original data set and the total variance over the transformed data set then these should be the same. However, in the latter case, the total variance is redistributed among the new variables unequally. Specifically, the first variable not only explains the most variance among the new variables, but the most variance a single variable can possibly explain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second side note, the fraction of variance explained from principal $j$ component is also equalt to: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\lambda_j}{\\sum_i \\lambda_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by now, we have a process that allows us to calculate the principal directions in the data set $\\mathbf{X}$. How can we use that in order to reduce the dimensions of the data set? The answer is that we don't have to retain the whole $\\mathbf{V}$ but instead only the eigenvectors that correspond to the largest eigenvalues $\\lambda_i$ or equivalently to the largest singular values $\\sigma_i$. The new data set then is given by the following transformation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{X}_{new} = \\mathbf{X}\\mathbf{V}_{L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{V}_{L}$ means that only the first $L$ columns of $\\mathbf{V}$ are retained and $L<<d$. Thus, $\\mathbf{V}_{L}$ is constrained to contain the first $L$ largest principal components. These components are uncorrelated (orthogonal) to the other principal components even if the input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated).\n",
    "This trasformation therefore reduces the number of features from $d$ to $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sumekf\"></a> Summary of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Princiapl Component Analysis or PCA is a technique to obtain the so-called principal components of a data set. The principal components correspond to the eigenvectors of the matrix $\\mathbf{X}^T  \\mathbf{X}$. This matrix is symmetric and thus the components are orthogoanl to each other. Therefore, we can use them to trasform the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first $k$ principal components (where can be 1, 2, 3 etc.) explain the most variance any $k$ variables can explain, and the last $m-k$ variables explain the least variance any variables can explain. By retaining only the first $k$ components we can reduce a $d-$dimensional data set to a $k-$dimensional set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing dimensionality reduction, one should bear in mind that we project from a higher dimensional subspace to a lower dimensional one. Thus, information is unavoidably lost. When it comes to PCA, if the number of variables is large, it becomes hard to interpret the principal components. Furthermore, the technique is mostly suitable when the associated features have a linear relationship among them. Finally, PCA is sensitive to  outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"motion_model\"></a> Test Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider the following toy example. The data set $\\mathbf{X}$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DynMat<real_t> X(6, 2);\n",
    "X(0,0) = -1.;\n",
    "X(0,1) = -1.;\n",
    "    \n",
    "X(1,0) = -2.;\n",
    "X(1,1) = -1.;\n",
    "    \n",
    "X(2,0) = -3.;\n",
    "X(2,1) = -2.;\n",
    "    \n",
    "X(3,0) = 1.;\n",
    "X(3,1) = 1.;\n",
    "    \n",
    "X(4,0) = 2.;\n",
    "X(4,1) = 1.;\n",
    "    \n",
    "X(5,0) = 3.;\n",
    "X(5,1) = 2.;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the empirical mean for each of the two columns is zero. The <a href=\"https://bitbucket.org/blaze-lib/blaze/src/master/\">Blaze</a> library that we use to represent matrices and vectors has support for SVD. We will use the following function in the code below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "template< typename MT1, bool SO, typename VT, bool TF, typename MT2, typename MT3 >\n",
    "void svd( const DenseMatrix<MT1,SO>& A, DenseMatrix<MT2,SO>& U,\n",
    "          DenseVector<VT,TF>& s, DenseMatrix<MT3,SO>& V );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above is rather simple. We will use the <a href=\"https://archive.ics.uci.edu/ml/datasets/wine\">wine</a> data set as a more complicated example. This data set has 178 examples and 12 features. We can load the data set by issuing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "auto data = kernel::load_wine_data_set(false);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will use the ```PCA``` class that helps us with maintaining the relevant information. Note that the class only transforms the supplied data set according to the transformation given above. However, it is the application's responsibility to scale the data appropriately if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"include_files\"></a> Include files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include \"cubic_engine/base/cubic_engine_types.h\"\n",
    "#include \"kernel/maths/matrix_utilities.h\"\n",
    "#include \"kernel/utilities/data_set_loaders.h\"\n",
    "#include \"kernel/maths/pca.h\"\n",
    "\n",
    "#include <iostream>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "namespace example\n",
    "{\n",
    "\n",
    "using cengine::uint_t;\n",
    "using cengine::real_t;\n",
    "using cengine::DynMat;\n",
    "using cengine::DynVec;\n",
    "\n",
    "void test_case_1(){\n",
    "\n",
    "    DynMat<real_t> X(6, 2);\n",
    "    X(0,0) = -1.;\n",
    "    X(0,1) = -1.;\n",
    "\n",
    "    X(1,0) = -2.;\n",
    "    X(1,1) = -1.;\n",
    "\n",
    "    X(2,0) = -3.;\n",
    "    X(2,1) = -2.;\n",
    "\n",
    "    X(3,0) = 1.;\n",
    "    X(3,1) = 1.;\n",
    "\n",
    "    X(4,0) = 2.;\n",
    "    X(4,1) = 1.;\n",
    "\n",
    "    X(5,0) = 3.;\n",
    "    X(5,1) = 2.;\n",
    "\n",
    "    // caluclate the sample variance\n",
    "    // of each of the 3 variables (columns)\n",
    "    auto col1 = kernel::get_column(X, 0);\n",
    "    auto col2 = kernel::get_column(X, 1);\n",
    "\n",
    "    auto col1_var = var(col1);\n",
    "    auto col2_var = var(col2);\n",
    "\n",
    "    std::cout<<\"Variable 1 variance: \"<<col1_var<<std::endl;\n",
    "    std::cout<<\"Variable 2 variance: \"<<col2_var<<std::endl;\n",
    "\n",
    "    // compute the total variance\n",
    "    auto total_var = col1_var + col2_var;\n",
    "\n",
    "    std::cout<<\"Total variance: \"<<total_var<<std::endl;\n",
    "\n",
    "    DynMat<real_t> U;\n",
    "    DynVec<real_t> s;\n",
    "    DynMat<real_t> V;\n",
    "\n",
    "    std::cout<<\"Variable 1 explains: \"<<col1_var/total_var<<std::endl;\n",
    "    std::cout<<\"Variable 2 explains: \"<<col2_var/total_var<<std::endl;\n",
    "\n",
    "    svd(X, U, s, V );\n",
    "\n",
    "    std::cout<<\"Singular values: \"<<s<<std::endl;\n",
    "\n",
    "    auto sum_eigen_values = 0.0;\n",
    "    for(uint_t v=0; v<s.size(); ++v){\n",
    "       sum_eigen_values += s[v]*s[v];\n",
    "    }\n",
    "\n",
    "    std::cout<<\"Sum eignenvalies: \"<<sum_eigen_values<<std::endl;\n",
    "    //std::cout<<\"Variable 1 variance: \"<<s[0]*s[0]<<std::endl;\n",
    "    //std::cout<<\"Variable 2 variance: \"<<s[1]*s[1]<<std::endl;\n",
    "    std::cout<<\"Variable 1 explains: \"<<(s[0]*s[0])/sum_eigen_values<<std::endl;\n",
    "    std::cout<<\"Variable 2 explains: \"<<(s[1]*s[1])/sum_eigen_values<<std::endl;\n",
    "\n",
    "    // Principal axes in feature space,\n",
    "    // representing the directions of maximum variance in the data.\n",
    "    // these are the columns of the V matrix\n",
    "    std::cout<<\"V matrix: \"<<V<<std::endl;\n",
    "\n",
    "    // reconstruct the data set with PCA\n",
    "    // The full principal components decomposition of\n",
    "    // X can be given as T= XW\n",
    "    DynMat<real_t> T = X*V;\n",
    "\n",
    "    // caluclate the sample variance\n",
    "    // of each of the 3 variables (columns)\n",
    "    auto pca_col1 = kernel::get_column(T, 0);\n",
    "    auto pca_col2 = kernel::get_column(T, 1);\n",
    "\n",
    "    auto pca_col1_var = var(pca_col1);\n",
    "    auto pca_col2_var = var(pca_col2);\n",
    "\n",
    "    std::cout<<\"PCA variable 1 variance: \"<<pca_col1_var<<std::endl;\n",
    "    std::cout<<\"PCA variable 2 variance: \"<<pca_col2_var<<std::endl;\n",
    "\n",
    "    // this should be the same at the total variance\n",
    "    // compute the total variance\n",
    "    auto pca_total_var = pca_col1_var + pca_col2_var;\n",
    "\n",
    "    std::cout<<\"PCA Total variance: \"<<pca_total_var<<std::endl;\n",
    "\n",
    "    std::cout<<\"PCA Variable 1 explains: \"<<pca_col1_var/pca_total_var<<std::endl;\n",
    "    std::cout<<\"PCA Variable 2 explains: \"<<pca_col2_var/pca_total_var<<std::endl;\n",
    "\n",
    "}\n",
    "\n",
    "void test_case_2(){\n",
    "\n",
    "    using  kernel::PCA;\n",
    "\n",
    "    // load the wine data set\n",
    "    auto data = kernel::load_wine_data_set(false);\n",
    "\n",
    "    // extract the column means\n",
    "    auto means = kernel::get_column_means(data.first);\n",
    "\n",
    "    // crenter the columns\n",
    "    kernel::center_columns(data.first, means);\n",
    "\n",
    "    auto variances = kernel::get_column_variances(data.first);\n",
    "    auto total_var = sum(variances);\n",
    "    std::cout<<\"Total variance: \"<<total_var<<std::endl;\n",
    "\n",
    "    for(uint_t c=0; c<variances.size(); ++c){\n",
    "        std::cout<<\"Variable: \"<<c<<\" explains: \"<<variances[c]/total_var<<std::endl;\n",
    "    }\n",
    "\n",
    "    // keep the first three components\n",
    "    // with the largest variance\n",
    "    PCA pca(3);\n",
    "\n",
    "    // transform the data\n",
    "    pca.fit(data.first);\n",
    "\n",
    "    auto singular_vals = pca.get_singular_values();\n",
    "    std::cout<<\"Singular values: \"<<singular_vals<<std::endl;\n",
    "\n",
    "    auto explained_var = pca.get_explained_variance();\n",
    "\n",
    "    for(uint_t c=0; c<explained_var.size(); ++c){\n",
    "        std::cout<<\"Component: \"<<c<<\" explains: \"<<explained_var[c]<<std::endl;\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "}\n",
    "\n",
    "int main() {\n",
    "   \n",
    "    using namespace example;\n",
    "    \n",
    "    try{\n",
    "\n",
    "        std::cout<<\"=========================\"<<std::endl;\n",
    "        std::cout<<\"Doing test 1\"<<std::endl;\n",
    "        std::cout<<\"=========================\"<<std::endl;\n",
    "        test_case_1();\n",
    "\n",
    "        std::cout<<\"=========================\"<<std::endl;\n",
    "        std::cout<<\"Doing test 2\"<<std::endl;\n",
    "        std::cout<<\"=========================\"<<std::endl;\n",
    "        test_case_2();\n",
    "    }\n",
    "    catch(std::runtime_error& e){\n",
    "        std::cerr<<\"Runtime error: \"\n",
    "                 <<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(std::logic_error& e){\n",
    "        std::cerr<<\"Logic error: \"\n",
    "                 <<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(...){\n",
    "        std::cerr<<\"Unknown exception was raised whilst running simulation.\"<<std::endl;\n",
    "    }\n",
    "   \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the driver code above we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "=========================\n",
    "Doing test 1\n",
    "=========================\n",
    "Variable 1 variance: 5.6\n",
    "Variable 2 variance: 2.4\n",
    "Total variance: 8\n",
    "Variable 1 explains: 0.7\n",
    "Variable 2 explains: 0.3\n",
    "Singular values: (     6.30061 )\n",
    "(    0.549804 )\n",
    "\n",
    "Sum eignenvalies: 40\n",
    "Variable 1 explains: 0.992443\n",
    "Variable 2 explains: 0.00755711\n",
    "V matrix: (    -0.838492    -0.544914 )\n",
    "(    -0.544914     0.838492 )\n",
    "\n",
    "PCA variable 1 variance: 7.93954\n",
    "PCA variable 2 variance: 0.0604569\n",
    "PCA Total variance: 8\n",
    "PCA Variable 1 explains: 0.992443\n",
    "PCA Variable 2 explains: 0.00755711\n",
    "=========================\n",
    "Doing test 2\n",
    "=========================\n",
    "Total variance: 224.788\n",
    "Variable: 0 explains: 0.00293193\n",
    "Variable: 1 explains: 0.00555198\n",
    "Variable: 2 explains: 0.000334826\n",
    "Variable: 3 explains: 0.0496143\n",
    "Variable: 4 explains: 0.907476\n",
    "Variable: 5 explains: 0.00174249\n",
    "Variable: 6 explains: 0.00443849\n",
    "Variable: 7 explains: 6.89034e-05\n",
    "Variable: 8 explains: 0.00145735\n",
    "Variable: 9 explains: 0.023909\n",
    "Variable: 10 explains: 0.000232419\n",
    "Variable: 11 explains: 0.0022425\n",
    "Singular values: (     190.221 )\n",
    "(     45.1776 )\n",
    "(     31.5194 )\n",
    "(     16.7921 )\n",
    "(     12.5484 )\n",
    "(     7.59523 )\n",
    "(      5.1764 )\n",
    "(     4.45796 )\n",
    "(     3.56478 )\n",
    "(     2.65211 )\n",
    "(     1.94634 )\n",
    "(     1.20831 )\n",
    "\n",
    "Component: 0 explains: 0.909437\n",
    "Component: 1 explains: 0.0512981\n",
    "Component: 2 explains: 0.0249696\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exe.cpp</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
