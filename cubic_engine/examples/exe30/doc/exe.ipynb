{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 30: PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Acknowledgements](#ackw)\n",
    "* [Overview](#overview) \n",
    "    * [Priiciapl Component Analysis](#ekf)\n",
    "    * [Summary PCA](#sumekf)\n",
    "    * [Test Case](#motion_model)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Principal Component Analysis</a> for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction refres to a number of techniques for reducing the dimensions associated with a data set. Consider, for example, a data set where each input point has five features. A dimensionality reduction technique can help us reduce the number of features to three or two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary reason behind why someone would like to reduce the number of dimensions of a data set is that a large number  of input features can cause poor performance of ML algorithms. This is frequently abbreviated as the curse of dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, often fewer input dimensions translate to fewer model parameters or simpler model structure in general. A model with too many parameters is likely to overfit the training set and therefore it may not perform well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, dimensionality reduction may also be appropriate when the variables in a dataset are noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can be thought of as fitting a p-dimensional ellipsoid to the data. Each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we find the axes of the ellipsoid? We first center the data around the origin by subtracting the mean of each variable from the dataset. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can be done either by performing a <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\"> Singular Value Decomposition</a> or by doing the following two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the data covariance (or correlation) matrix of the original data\n",
    "2. Perform eigenvalue decomposition on the covariance matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors  should be eignevectors of the matrix $\\mathbf{X}^T\\mathbf{X}$. Thus they should satisfy the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{X}^T  \\mathbf{X}\\mathbf{w}_j = \\lambda_j\\mathbf{w}_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\mathbf{X}^T  \\mathbf{X}$ is symmetric and therefore the eigenvectors are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD of a matrix $\\mathbf{P}$ has the following form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{P} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{\\Sigma}$ is  a diagonal-like matrix containing the singular values $\\sigma_i$ and $\\mathbf{V}$ is an orthogonal matrix. The eigenvalues $\\lambda_i$ of $\\mathbf{P}^T\\mathbf{P}$ and the singular values $\\sigma_i$ of $\\mathbf{P}$ are connected via"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda_i = \\sigma_{i}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the columns of $\\mathbf{V}$ are the eigenvectors for $\\mathbf{P}^T\\mathbf{P}$. Moreover, the SVD process orders the singular values according to their size: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can use as $\\mathbf{w}_1$ the first column of $\\mathbf{V}$ as this will be an eigenvector for the largest singular value $\\sigma_1 = \\sqrt{\\lambda_1}$. Similarly for the rest $\\mathbf{w}_i$. The columns of $\\mathbf{V}$ are called the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total variance is the sum of variances of all individual principal components. The fraction of variance explained by a principal component is the ratio between the variance of that principal component and the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sumekf\"></a> Summary of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the first $k$ principal components (where can be 1, 2, 3 etc.) explain the most variance any $k$ variables can explain, and the last $k$ variables explain the least variance any variables can explain, under some general restrictions. (The restrictions ensure, for example, that we cannot adjust a variableâ€™s explained variance simply by scaling it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"motion_model\"></a> Test Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider the following toy example. The data set $\\mathbf{X}$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DynMat<real_t> X(6, 2);\n",
    "X(0,0) = -1.;\n",
    "X(0,1) = -1.;\n",
    "    \n",
    "X(1,0) = -2.;\n",
    "X(1,1) = -1.;\n",
    "    \n",
    "X(2,0) = -3.;\n",
    "X(2,1) = -2.;\n",
    "    \n",
    "X(3,0) = 1.;\n",
    "X(3,1) = 1.;\n",
    "    \n",
    "X(4,0) = 2.;\n",
    "X(4,1) = 1.;\n",
    "    \n",
    "X(5,0) = 3.;\n",
    "X(5,1) = 2.;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the empirical mean for each of the two columns is zero. The <a href=\"https://bitbucket.org/blaze-lib/blaze/src/master/\">Blaze</a> library that we use to represent matrices and vectors has support for SVD. We will use the following function in the code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "template< typename MT1, bool SO, typename VT, bool TF, typename MT2, typename MT3 >\n",
    "void svd( const DenseMatrix<MT1,SO>& A, DenseMatrix<MT2,SO>& U,\n",
    "          DenseVector<VT,TF>& s, DenseMatrix<MT3,SO>& V );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"include_files\"></a> Include files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include \"cubic_engine/base/cubic_engine_types.h\"\n",
    "#include \"kernel/maths/matrix_utilities.h\"\n",
    "#include <cmath>\n",
    "#include <iostream>\n",
    "#include <tuple>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "namespace example\n",
    "{\n",
    "\n",
    "using cengine::uint_t;\n",
    "using cengine::real_t;\n",
    "using cengine::DynMat;\n",
    "using cengine::DynVec;\n",
    "\n",
    "}\n",
    "\n",
    "int main() {\n",
    "   \n",
    "    using namespace example;\n",
    "    \n",
    "    DynMat<real_t> X(6, 2);\n",
    "    X(0,0) = -1.;\n",
    "    X(0,1) = -1.;\n",
    "    \n",
    "    X(1,0) = -2.;\n",
    "    X(1,1) = -1.;\n",
    "    \n",
    "    X(2,0) = -3.;\n",
    "    X(2,1) = -2.;\n",
    "    \n",
    "    X(3,0) = 1.;\n",
    "    X(3,1) = 1.;\n",
    "    \n",
    "    X(4,0) = 2.;\n",
    "    X(4,1) = 1.;\n",
    "    \n",
    "    X(5,0) = 3.;\n",
    "    X(5,1) = 2.;\n",
    "\n",
    "    // caluclate the sample variance\n",
    "    // of each of the 3 variables (columns)\n",
    "    auto col1 = kernel::get_column(X, 0);\n",
    "    auto col2 = kernel::get_column(X, 1);\n",
    "\n",
    "    auto col1_var = var(col1);\n",
    "    auto col2_var = var(col2);\n",
    "\n",
    "    std::cout<<\"Variable 1 variance: \"<<col1_var<<std::endl;\n",
    "    std::cout<<\"Variable 2 variance: \"<<col2_var<<std::endl;\n",
    "\n",
    "    // compute the total variance\n",
    "    auto total_var = col1_var + col2_var;\n",
    "\n",
    "    std::cout<<\"Total variance: \"<<total_var<<std::endl;\n",
    "\n",
    "    DynMat<real_t> U;\n",
    "    DynVec<real_t> s;\n",
    "    DynMat<real_t> V;\n",
    "    try{\n",
    "\n",
    "        std::cout<<\"Variable 1 explains: \"<<col1_var/total_var<<std::endl;\n",
    "        std::cout<<\"Variable 2 explains: \"<<col2_var/total_var<<std::endl;\n",
    "\n",
    "        svd(X, U, s, V );\n",
    "\n",
    "        std::cout<<\"Singular values: \"<<s<<std::endl;\n",
    "\n",
    "        // Principal axes in feature space,\n",
    "        // representing the directions of maximum variance in the data.\n",
    "        // these are the columns of the V matrix\n",
    "        std::cout<<\"V matrix: \"<<V<<std::endl;\n",
    "\n",
    "        // reconstruct the data set with PCA\n",
    "        // The full principal components decomposition of\n",
    "        // X can be given as T= XW\n",
    "        DynMat<real_t> T = X*V;\n",
    "\n",
    "        // caluclate the sample variance\n",
    "        // of each of the 3 variables (columns)\n",
    "        auto pca_col1 = kernel::get_column(T, 0);\n",
    "        auto pca_col2 = kernel::get_column(T, 1);\n",
    "\n",
    "        auto pca_col1_var = var(pca_col1);\n",
    "        auto pca_col2_var = var(pca_col2);\n",
    "\n",
    "        std::cout<<\"PCA variable 1 variance: \"<<pca_col1_var<<std::endl;\n",
    "        std::cout<<\"PCA variable 2 variance: \"<<pca_col2_var<<std::endl;\n",
    "\n",
    "        // this should be the same at the total variance\n",
    "        // compute the total variance\n",
    "        auto pca_total_var = pca_col1_var + pca_col2_var;\n",
    "\n",
    "        std::cout<<\"PCA Total variance: \"<<pca_total_var<<std::endl;\n",
    "\n",
    "        std::cout<<\"PCA Variable 1 explains: \"<<pca_col1_var/pca_total_var<<std::endl;\n",
    "        std::cout<<\"PCA Variable 2 explains: \"<<pca_col2_var/pca_total_var<<std::endl;\n",
    "\n",
    "\n",
    "    }\n",
    "    catch(std::runtime_error& e){\n",
    "        std::cerr<<\"Runtime error: \"\n",
    "                 <<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(std::logic_error& e){\n",
    "        std::cerr<<\"Logic error: \"\n",
    "                 <<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(...){\n",
    "        std::cerr<<\"Unknown exception was raised whilst running simulation.\"<<std::endl;\n",
    "    }\n",
    "   \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the driver code above we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Variable 1 variance: 5.6\n",
    "Variable 2 variance: 2.4\n",
    "Total variance: 8\n",
    "Variable 1 explains: 0.7\n",
    "Variable 2 explains: 0.3\n",
    "Singular values: (     6.30061 )\n",
    "(    0.549804 )\n",
    "\n",
    "V matrix: (    -0.838492    -0.544914 )\n",
    "(    -0.544914     0.838492 )\n",
    "\n",
    "PCA variable 1 variance: 7.93954\n",
    "PCA variable 2 variance: 0.0604569\n",
    "PCA Total variance: 8\n",
    "PCA Variable 1 explains: 0.992443\n",
    "PCA Variable 2 explains: 0.00755711\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exe.cpp</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
