{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 26: Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Acknowledgements](#ackw)\n",
    "* [Overview](#overview) \n",
    "    * [Policy iteration algorithm](#ekf)\n",
    "    * [Test case](#motion_model)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ackw\"></a> Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the content in this notebook is taken from the book ```Reinforcement Learning An introduction``` by Sutton and Barton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss the so-called Policy Iteration algorithm for a finite MDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Policy iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind policy iteration is rather simple; once a policy $\\pi$ has been improved using $V_{\\pi}$ to yield a better policy say $\\pi_1$ we can then compute $V_{\\pi_1}$ and improve it again to yield an even better policy say $\\pi_2$. This is of course true provided that the used policy is not optimal already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process will give us a sequence of monotonically improving policies and value fuctions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\pi_0 \\rightarrow V_{\\pi_0} \\rightarrow \\pi_1 \\rightarrow V_{\\pi_1}\\rightarrow \\pi_2 \\cdots \\pi_{*} \\rightarrow V_{\\pi_{*}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step $\\pi_i \\rightarrow V_{\\pi_i}$ is an evaluation step. On the other hand the step $V_{\\pi_i} \\rightarrow \\pi_{i+1} $ is an improvement step. We know that we can use iterative policy evaluation at this step. Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are assuming a finite MDP, this means that  only a finite number of policies exists. Thus, this process must converge to an optimal policy and optimal value function in a finite number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way of finding an optimal policy is called policy iteration. The algorithm is shown in the image below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"policy_iteration.png\"\n",
    "     alt=\"Policy Iteration\"\n",
    "     style=\"float: left; margin-right: 10px; width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the image is taken from the book ```Reinforcement Learning An introduction``` by Sutton and Barto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each policy evaluation, itself an iterative computation,\n",
    "is started with the value function for the previous policy. This typically results in a great\n",
    "increase in the speed of convergence of policy evaluation (presumably because the value\n",
    "function changes little from one policy to the next)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"motion_model\"></a> Test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common test case for SARSA is the so-called Cliff world. The used world is shown in the image below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cliff_world.png\"\n",
    "     alt=\"Cliff World\"\n",
    "     style=\"float: left; margin-right: 10px; width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a standard un-discounted i.e. $\\gamma = 1$, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is -1 on all transitions except those into the region marked Cliff. Stepping into this region incurs a reward of optimal path -100 and sends the agent instantly back to the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"include_files\"></a> Include files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include \"cubic_engine/base/cubic_engine_types.h\"\n",
    "#include \"kernel/utilities/csv_file_writer.h\"\n",
    "#include \"kernel/base/kernel_consts.h\"\n",
    "#include \"kernel/utilities/csv_file_writer.h\"\n",
    "#include \"cubic_engine/rl/worlds/cliff_world.h\"\n",
    "#include \"cubic_engine/rl/worlds/grid_world_action_space.h\"\n",
    "#include \"cubic_engine/rl/tabular_sarsa_learning.h\"\n",
    "#include \"cubic_engine/rl/reward_table.h\"\n",
    "\n",
    "#include <cmath>\n",
    "#include <utility>\n",
    "#include <tuple>\n",
    "#include <iostream>\n",
    "#include <random>\n",
    "#include <algorithm>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "int main(){\n",
    "\n",
    "    using cengine::uint_t;\n",
    "    using cengine::real_t;\n",
    "    using cengine::rl::worlds::CliffWorld;\n",
    "    using cengine::rl::worlds::GridWorldAction;\n",
    "    using cengine::rl::SarsaTableLearning;\n",
    "    using cengine::rl::SarsaLearningInput;\n",
    "    using cengine::rl::RewardTable;\n",
    "    using kernel::CSVWriter;\n",
    "\n",
    "    try{\n",
    "\n",
    "        typedef CliffWorld world_t;\n",
    "        typedef world_t::state_t state_t;\n",
    "\n",
    "        /// the world of the agent\n",
    "        CliffWorld world;\n",
    "        world.create_world();\n",
    "\n",
    "        std::cout<<\"Number of states: \"<<world.n_states()<<std::endl;\n",
    "\n",
    "        state_t start(0);\n",
    "        state_t goal(11);\n",
    "\n",
    "        /// simulation parameters\n",
    "        /// number of episodes for the agent to learn.\n",
    "        const uint_t N_ITERATIONS = 500;\n",
    "        const real_t ETA = 0.1;\n",
    "        const real_t EPSILON = 0.1;\n",
    "        const real_t GAMMA = 1.0;\n",
    "        const real_t PENALTY = -100.0;\n",
    "\n",
    "        SarsaLearningInput qinput={ETA, EPSILON, GAMMA, true, true};\n",
    "        SarsaTableLearning<world_t> sarsalearner(std::move(qinput));\n",
    "\n",
    "        CSVWriter writer(\"agent_rewards.csv\", ',', true);\n",
    "        writer.write_column_names({\"Episode\", \"Reward\"}, true);\n",
    "\n",
    "        std::vector<real_t> row(2);\n",
    "        sarsalearner.initialize(world, PENALTY);\n",
    "\n",
    "        auto& table = sarsalearner.get_table();\n",
    "        table.save_to_csv(\"table_rewards\" + std::to_string(0) + \".csv\");\n",
    "\n",
    "        for(uint_t episode=0; episode < N_ITERATIONS; ++episode){\n",
    "\n",
    "            std::cout<<\"At episode: \"<<episode<<std::endl;\n",
    "            world.restart(start, goal);\n",
    "            auto result = sarsalearner.train(goal);\n",
    "\n",
    "            /// the total reward the agent obtained\n",
    "            /// in this episode\n",
    "            auto reward = result.total_reward;\n",
    "            writer.write_row(std::make_tuple(episode, reward));\n",
    "            std::cout<<\"At episode: \"<<episode<<\" total reward: \"<<reward<<std::endl;\n",
    "\n",
    "            if(episode == N_ITERATIONS - 1){\n",
    "                auto& table = sarsalearner.get_table();\n",
    "                table.save_to_csv(\"table_rewards\" + std::to_string(episode) + \".csv\");\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    catch(std::exception& e){\n",
    "\n",
    "        std::cerr<<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(...){\n",
    "\n",
    "        std::cerr<<\"Unknown exception occured\"<<std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "...\n",
    "\n",
    "Taking action: SOUTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 27 and action: SOUTH -1\n",
    "\tNext action: NORTH\n",
    "\tNext state: 15\n",
    "\tSetting for state: 27 and action: SOUTH to value: -1\n",
    "\tAt iteration: 215\n",
    "\tCurrent state: 15\n",
    "\tTaking action: NORTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 15 and action: NORTH -1\n",
    "\tNext action: SOUTH\n",
    "\tNext state: 27\n",
    "\tSetting for state: 15 and action: NORTH to value: -1\n",
    "\tAt iteration: 216\n",
    "\tCurrent state: 27\n",
    "\tTaking action: SOUTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 27 and action: SOUTH -1\n",
    "\tNext action: NORTH\n",
    "\tNext state: 15\n",
    "\tSetting for state: 27 and action: SOUTH to value: -1\n",
    "\tAt iteration: 217\n",
    "\tCurrent state: 15\n",
    "\tTaking action: NORTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 15 and action: NORTH -1\n",
    "\tNext action: SOUTH\n",
    "\tNext state: 27\n",
    "\tSetting for state: 15 and action: NORTH to value: -1\n",
    "\tAt iteration: 218\n",
    "\tCurrent state: 27\n",
    "\tTaking action: SOUTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 27 and action: SOUTH -1\n",
    "\tNext action: NORTH\n",
    "\tNext state: 15\n",
    "\tSetting for state: 27 and action: SOUTH to value: -1\n",
    "\tAt iteration: 219\n",
    "\tCurrent state: 15\n",
    "\tTaking action: NORTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 15 and action: NORTH -1\n",
    "\tNext action: SOUTH\n",
    "\tNext state: 27\n",
    "\tSetting for state: 15 and action: NORTH to value: -1\n",
    "\tAt iteration: 220\n",
    "\tCurrent state: 27\n",
    "\tTaking action: SOUTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 27 and action: SOUTH -1\n",
    "\tNext action: NORTH\n",
    "\tRecalculated next action: SOUTH\n",
    "\tNext state: 15\n",
    "\tSetting for state: 27 and action: SOUTH to value: -1\n",
    "\tAt iteration: 221\n",
    "\tCurrent state: 15\n",
    "\tTaking action: SOUTH\n",
    "WORLD FINISHED AT STATE: 15 AND ACTION: SOUTH\n",
    "At episode: 498 total reward: -220\n",
    "At episode: 499\n",
    "\tAt iteration: 1\n",
    "\tCurrent state: 0\n",
    "\tTaking action: SOUTH\n",
    "WORLD FINISHED AT STATE: 0 AND ACTION: SOUTH\n",
    "At episode: 499 total reward: 0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following images shown the sum of rewards achieved by the algorithm for various values of $\\epsilon$. We see that as $\\epsilon$ is increased more exploration occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"e_0_1.png\"\n",
    "     alt=\"Total Rewards\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"e_0_2.png\"\n",
    "     alt=\"Total Rewards\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"e_0_3.png\"\n",
    "     alt=\"Total Rewards\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exe.cpp</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
