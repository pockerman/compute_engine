{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 26: Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Acknowledgements](#ackw)\n",
    "* [Overview](#overview) \n",
    "    * [Policy iteration algorithm](#ekf)\n",
    "    * [Test case](#motion_model)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ackw\"></a> Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the content in this notebook is taken from the book ```Reinforcement Learning An introduction``` by Sutton and Barto. Note that the C++ code is an adaptation of the Python code in the repository <a href=\"https://github.com/ShangtongZhang/reinforcement-learning-an-introduction\">ShangtongZhang/reinforcement-learning-an-introduction</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss the so-called Policy Iteration algorithm for a finite MDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Policy iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind policy iteration is rather simple; once a policy $\\pi$ has been improved using $V_{\\pi}$ to yield a better policy say $\\pi_1$ we can then compute $V_{\\pi_1}$ and improve it again to yield an even better policy say $\\pi_2$. This is of course true provided that the used policy is not optimal already. This process will give us a sequence of monotonically improving policies and value fuctions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\pi_0 \\rightarrow V_{\\pi_0} \\rightarrow \\pi_1 \\rightarrow V_{\\pi_1}\\rightarrow \\pi_2 \\cdots \\pi_{*} \\rightarrow V_{\\pi_{*}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step $\\pi_i \\rightarrow V_{\\pi_i}$ is an evaluation step. On the other hand the step $V_{\\pi_i} \\rightarrow \\pi_{i+1} $ is an improvement step. We know that we can use iterative policy evaluation at this step. Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are assuming a finite MDP, this means that  only a finite number of policies exists. Thus, this process must converge to an optimal policy and optimal value function in a finite number of iterations. This way of finding an optimal policy is called **policy iteration** (see ```Reinforcement Learning An introduction``` by Sutton and Barton).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each policy evaluation is itself an iterative computation. It\n",
    "is started with the value function from the previous policy. This most of the times results in a great\n",
    "increase in the speed of convergence of policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"motion_model\"></a> Test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test case we will consider is the own described in the relevant section of the book ```Reinforcement Learning An introduction``` by Sutton and Barto. Concretely, the test case  is example 4.2. The test case is as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jack manages two locations, denoted by $L_1$ and $L_2$ respectively, for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars.  If Jack has a car available, herents it out and is credited \\\\$10 by the national company.  If he is out of cars at that location, then thebusiness is lost.  Cars become available for renting the day after they are returned.  To help ensure thatcars are available where they are needed, Jack can move them between the two locations overnight, ata cost of \\\\$2 per car moved.  We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\lambda^n}{n!}e^{-\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in onenight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S=(S_1,S_2)$ be the tuple that denotes the number of cars at locations $L_1$ and $L_2$. The action set is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A=\\{0,1,2,3,4,5\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount rate is set to  $\\gamma = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"include_files\"></a> Include files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include \"cubic_engine/base/cubic_engine_types.h\"\n",
    "#include \"kernel/utilities/csv_file_writer.h\"\n",
    "#include \"kernel/base/kernel_consts.h\"\n",
    "#include \"kernel/utilities/array_utils.h\"\n",
    "\n",
    "#include <cmath>\n",
    "#include <utility>\n",
    "#include <tuple>\n",
    "#include <iostream>\n",
    "#include <random>\n",
    "#include <algorithm>\n",
    "#include <cmath>\n",
    "#include <limits>\n",
    "#include <map>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "namespace example\n",
    "{\n",
    "using cengine::uint_t;\n",
    "using cengine::real_t;\n",
    "using cengine::DynMat;\n",
    "using kernel::CSVWriter;\n",
    "\n",
    "// max number of cars in each location\n",
    "const int MAX_CARS = 20;\n",
    "\n",
    "// max number of cars to move during night\n",
    "const uint_t MAX_MOVE_OF_CARS = 5;\n",
    "\n",
    "// expectation for rental requests in first location\n",
    "const uint_t RENTAL_REQUEST_FIRST_LOC = 3;\n",
    "\n",
    "// expectation for rental requests in second location\n",
    "const uint_t RENTAL_REQUEST_SECOND_LOC = 4;\n",
    "\n",
    "// expectation for # of cars returned in first location\n",
    "const int RETURNS_FIRST_LOC = 3;\n",
    "\n",
    "// expectation for # of cars returned in second location\n",
    "const int RETURNS_SECOND_LOC = 2;\n",
    "\n",
    "// discount factor\n",
    "const real_t DISCOUNT = 0.9;\n",
    "\n",
    "// credit earned by a car\n",
    "const real_t RENTAL_CREDIT = 10;\n",
    "\n",
    "// cost of moving a car\n",
    "const real_t MOVE_CAR_COST = 2;\n",
    "\n",
    "// all possible actions\n",
    "int actions[] = {-5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5};\n",
    "\n",
    "// An up bound for poisson distribution\n",
    "// If n is greater than this value, then the probability\n",
    "// of getting n is truncated to 0\n",
    "const uint_t POISSON_UPPER_BOUND = 11;\n",
    "\n",
    "// cache for the Poisson values\n",
    "std::map<int, real_t> poisson_cache;\n",
    "\n",
    "real_t poisson_pmf(int  k, real_t lambda) {\n",
    "    return std::exp(k * std::log(lambda) - std::lgamma(k + 1.0) - lambda);\n",
    "  }\n",
    "\n",
    "real_t poisson_probability(int n, int lam ){\n",
    "\n",
    "    auto key = n * 10 + lam;\n",
    "    auto itr = poisson_cache.find(key);\n",
    "    if(itr == poisson_cache.end()){\n",
    "        poisson_cache[key] = poisson_pmf(n, lam);\n",
    "    }\n",
    "\n",
    "    return poisson_cache[key];\n",
    "}\n",
    "\n",
    "real_t expected_return(int state[], int action, DynMat<real_t>& state_value, bool constant_returned_cars){\n",
    "\n",
    "    // initailize total return\n",
    "    real_t returns = 0.0;\n",
    "\n",
    "    // cost for moving cars\n",
    "    returns -= MOVE_CAR_COST * std::abs(action);\n",
    "\n",
    "    // moving cars\n",
    "    int NUM_OF_CARS_FIRST_LOC = std::min(state[0] - action, MAX_CARS);\n",
    "    int NUM_OF_CARS_SECOND_LOC = std::min(state[1] + action, MAX_CARS);\n",
    "\n",
    "    // go through all possible rental requests\n",
    "    for(int rental_request_first_loc=0;  rental_request_first_loc<POISSON_UPPER_BOUND; ++rental_request_first_loc){\n",
    "        for(int rental_request_second_loc=0;  rental_request_second_loc<POISSON_UPPER_BOUND; ++rental_request_second_loc){\n",
    "\n",
    "            // probability for current combination of rental requests\n",
    "            auto prob = poisson_probability(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC);\n",
    "            prob *= poisson_probability(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC);\n",
    "\n",
    "            auto num_of_cars_first_loc = NUM_OF_CARS_FIRST_LOC;\n",
    "            auto num_of_cars_second_loc = NUM_OF_CARS_SECOND_LOC;\n",
    "\n",
    "            // valid rental requests should be less than actual # of cars\n",
    "            auto valid_rental_first_loc = std::min(num_of_cars_first_loc, rental_request_first_loc);\n",
    "            auto valid_rental_second_loc = std::min(num_of_cars_second_loc, rental_request_second_loc);\n",
    "\n",
    "            // get credits for renting\n",
    "            auto reward = (valid_rental_first_loc + valid_rental_second_loc) * RENTAL_CREDIT;\n",
    "            num_of_cars_first_loc -= valid_rental_first_loc;\n",
    "            num_of_cars_second_loc -= valid_rental_second_loc;\n",
    "\n",
    "            if( constant_returned_cars){\n",
    "                // get returned cars, those cars can be used for renting tomorrow\n",
    "                auto returned_cars_first_loc = RETURNS_FIRST_LOC;\n",
    "                auto returned_cars_second_loc = RETURNS_SECOND_LOC;\n",
    "                num_of_cars_first_loc = std::min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS);\n",
    "                num_of_cars_second_loc = std::min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS);\n",
    "                returns += prob * (reward + DISCOUNT * state_value(num_of_cars_first_loc, num_of_cars_second_loc));\n",
    "            }\n",
    "            else{\n",
    "                for(int returned_cars_first_loc=0; returned_cars_first_loc<POISSON_UPPER_BOUND; ++returned_cars_first_loc){\n",
    "                    for( int returned_cars_second_loc=0; returned_cars_second_loc<POISSON_UPPER_BOUND; ++returned_cars_second_loc){\n",
    "\n",
    "                        auto prob_return = poisson_probability(\n",
    "                            returned_cars_first_loc, RETURNS_FIRST_LOC) * poisson_probability(returned_cars_second_loc, RETURNS_SECOND_LOC);\n",
    "                        auto num_of_cars_first_loc_ = std::min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS);\n",
    "                        auto num_of_cars_second_loc_ = std::min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS);\n",
    "                        auto prob_ = prob_return * prob;\n",
    "                        returns += prob_ * (reward + DISCOUNT *\n",
    "                                            state_value(num_of_cars_first_loc_, num_of_cars_second_loc_));\n",
    "                    }\n",
    "                }\n",
    "           }\n",
    "          }\n",
    "         }\n",
    "    return returns;\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "void simulate(bool constant_returned_cars){\n",
    "\n",
    "    DynMat<real_t> value(MAX_CARS + 1, MAX_CARS + 1, 0.);\n",
    "    DynMat<int> policy(MAX_CARS + 1, MAX_CARS + 1, -1);\n",
    "\n",
    "    // how many iteration have we performed\n",
    "    uint_t iterations = 0;\n",
    "\n",
    "    while(true){\n",
    "\n",
    "        // policy evaluation (in-place)\n",
    "        while(true) {\n",
    "\n",
    "           // a copy of the current value function\n",
    "           auto old_value = value;\n",
    "\n",
    "           for(int i=0; i<(MAX_CARS + 1); ++i){\n",
    "               for(int j=0; j<(MAX_CARS + 1); ++j){\n",
    "\n",
    "                   int local_state[] = {i,j};\n",
    "                   auto new_state_value = expected_return(local_state, policy(i,j),\n",
    "                                                          value, constant_returned_cars);\n",
    "                   value(i,j) = new_state_value;\n",
    "               }\n",
    "\n",
    "               auto  max_value_change = max(abs(old_value - value));\n",
    "               std::cout<<\"max value change: \"<<max_value_change<<std::endl;\n",
    "               if(max_value_change < 1e-4){\n",
    "                    break;\n",
    "               }\n",
    "           }\n",
    "        }\n",
    "\n",
    "        // policy improvement\n",
    "        bool policy_stable = true;\n",
    "\n",
    "        for(int i=0; i<(MAX_CARS + 1); ++i){\n",
    "            for(int j=0; j<(MAX_CARS + 1); ++j){\n",
    "\n",
    "                auto old_action = policy(i, j);\n",
    "                std::vector<real_t> action_returns;\n",
    "\n",
    "                for(uint_t a=0; a<11; ++a){\n",
    "\n",
    "                    auto action = actions[a];\n",
    "                    if((0 <= action <= i) || (-j <= action <= 0)){\n",
    "                        int local_state[] = {i,j};\n",
    "                        action_returns.push_back(expected_return(local_state, action,\n",
    "                                                                 value, constant_returned_cars));\n",
    "                    }\n",
    "                    else{\n",
    "                        action_returns.push_back(std::numeric_limits<real_t>::min());\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                auto new_action = actions[kernel::utils::arg_max(action_returns)];\n",
    "                policy(i, j) = new_action;\n",
    "                if( policy_stable && old_action != new_action){\n",
    "                        policy_stable = false;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if(policy_stable){\n",
    "            break;\n",
    "        }\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "int main(){\n",
    "\n",
    "\n",
    "    using namespace example;\n",
    "\n",
    "\n",
    "    try{\n",
    "        simulate(true);\n",
    "    }\n",
    "    catch(std::exception& e){\n",
    "\n",
    "        std::cerr<<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(...){\n",
    "\n",
    "        std::cerr<<\"Unknown exception occured\"<<std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following images shown the sum of rewards achieved by the algorithm for various values of $\\epsilon$. We see that as $\\epsilon$ is increased more exploration occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exe.cpp</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
