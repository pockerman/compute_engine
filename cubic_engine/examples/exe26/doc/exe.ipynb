{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 26: Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Acknowledgements](#ackw)\n",
    "* [Overview](#overview) \n",
    "    * [Policy iteration algorithm](#ekf)\n",
    "    * [Test case](#motion_model)\n",
    "* [Include files](#include_files)\n",
    "* [The main function](#m_func)\n",
    "* [Results](#results)\n",
    "* [Source Code](#source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ackw\"></a> Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the content in this notebook is taken from the book ```Reinforcement Learning An introduction``` by Sutton and Barto. Note that the C++ code is an adaptation of the Python code in the repository <a href=\"https://github.com/ShangtongZhang/reinforcement-learning-an-introduction\">ShangtongZhang/reinforcement-learning-an-introduction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss the so-called Policy Iteration algorithm for a finite MDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ekf\"></a> Policy iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind policy iteration is rather simple; once a policy $\\pi$ has been improved using $V_{\\pi}$ to yield a better policy say $\\pi_1$ we can then compute $V_{\\pi_1}$ and improve it again to yield an even better policy say $\\pi_2$. This is of course true provided that the used policy is not optimal already. This process will give us a sequence of monotonically improving policies and value fuctions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\pi_0 \\rightarrow V_{\\pi_0} \\rightarrow \\pi_1 \\rightarrow V_{\\pi_1}\\rightarrow \\pi_2 \\cdots \\pi_{*} \\rightarrow V_{\\pi_{*}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step $\\pi_i \\rightarrow V_{\\pi_i}$ is an evaluation step. On the other hand the step $V_{\\pi_i} \\rightarrow \\pi_{i+1} $ is an improvement step. We know that we can use iterative policy evaluation at this step. Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are assuming a finite MDP, this means that  only a finite number of policies exists. Thus, this process must converge to an optimal policy and optimal value function in a finite number of iterations. This way of finding an optimal policy is called **policy iteration** (see ```Reinforcement Learning An introduction``` by Sutton and Barton). The algorithm is shown in the image below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"policy_iteration.png\"\n",
    "     alt=\"Policy Iteration\"\n",
    "     style=\"float: left; margin-right: 10px; width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the image is taken from the book ```Reinforcement Learning An introduction``` by Sutton and Barto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each policy evaluation, itself an iterative computation,\n",
    "is started with the value function for the previous policy. This typically results in a great\n",
    "increase in the speed of convergence of policy evaluation (presumably because the value\n",
    "function changes little from one policy to the next)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"motion_model\"></a> Test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test case we will consider is the own described in the relevant section of the book ```Reinforcement Learning An introduction``` by Sutton and Barto. Concretely, the test case  is example 4.2. The test case is as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jack manages two locations, denoted by $L_1$ and $L_2$ respectively, for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars.  If Jack has a car available, herents it out and is credited \\\\$10 by the national company.  If he is out of cars at that location, then thebusiness is lost.  Cars become available for renting the day after they are returned.  To help ensure thatcars are available where they are needed, Jack can move them between the two locations overnight, ata cost of \\\\$2 per car moved.  We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\lambda^n}{n!}e^{-\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in onenight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S=(S_1,S_2)$ be the tuple that denotes the number of cars at locations $L_1$ and $L_2$. The action set is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A=\\{0,1,2,3,4,5\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cliff_world.png\"\n",
    "     alt=\"Cliff World\"\n",
    "     style=\"float: left; margin-right: 10px; width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount rate is set to  $\\gamma = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"include_files\"></a> Include files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include \"cubic_engine/base/cubic_engine_types.h\"\n",
    "#include \"kernel/utilities/csv_file_writer.h\"\n",
    "#include \"kernel/base/kernel_consts.h\"\n",
    "#include \"kernel/utilities/csv_file_writer.h\"\n",
    "#include \"cubic_engine/rl/worlds/cliff_world.h\"\n",
    "#include \"cubic_engine/rl/worlds/grid_world_action_space.h\"\n",
    "#include \"cubic_engine/rl/tabular_sarsa_learning.h\"\n",
    "#include \"cubic_engine/rl/reward_table.h\"\n",
    "\n",
    "#include <cmath>\n",
    "#include <utility>\n",
    "#include <tuple>\n",
    "#include <iostream>\n",
    "#include <random>\n",
    "#include <algorithm>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"m_func\"></a> The main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "int main(){\n",
    "\n",
    "    using cengine::uint_t;\n",
    "    using cengine::real_t;\n",
    "    using cengine::rl::worlds::CliffWorld;\n",
    "    using cengine::rl::worlds::GridWorldAction;\n",
    "    using cengine::rl::SarsaTableLearning;\n",
    "    using cengine::rl::SarsaLearningInput;\n",
    "    using cengine::rl::RewardTable;\n",
    "    using kernel::CSVWriter;\n",
    "\n",
    "    try{\n",
    "\n",
    "        typedef CliffWorld world_t;\n",
    "        typedef world_t::state_t state_t;\n",
    "\n",
    "        /// the world of the agent\n",
    "        CliffWorld world;\n",
    "        world.create_world();\n",
    "\n",
    "        std::cout<<\"Number of states: \"<<world.n_states()<<std::endl;\n",
    "\n",
    "        state_t start(0);\n",
    "        state_t goal(11);\n",
    "\n",
    "        /// simulation parameters\n",
    "        /// number of episodes for the agent to learn.\n",
    "        const uint_t N_ITERATIONS = 500;\n",
    "        const real_t ETA = 0.1;\n",
    "        const real_t EPSILON = 0.1;\n",
    "        const real_t GAMMA = 1.0;\n",
    "        const real_t PENALTY = -100.0;\n",
    "\n",
    "        SarsaLearningInput qinput={ETA, EPSILON, GAMMA, true, true};\n",
    "        SarsaTableLearning<world_t> sarsalearner(std::move(qinput));\n",
    "\n",
    "        CSVWriter writer(\"agent_rewards.csv\", ',', true);\n",
    "        writer.write_column_names({\"Episode\", \"Reward\"}, true);\n",
    "\n",
    "        std::vector<real_t> row(2);\n",
    "        sarsalearner.initialize(world, PENALTY);\n",
    "\n",
    "        auto& table = sarsalearner.get_table();\n",
    "        table.save_to_csv(\"table_rewards\" + std::to_string(0) + \".csv\");\n",
    "\n",
    "        for(uint_t episode=0; episode < N_ITERATIONS; ++episode){\n",
    "\n",
    "            std::cout<<\"At episode: \"<<episode<<std::endl;\n",
    "            world.restart(start, goal);\n",
    "            auto result = sarsalearner.train(goal);\n",
    "\n",
    "            /// the total reward the agent obtained\n",
    "            /// in this episode\n",
    "            auto reward = result.total_reward;\n",
    "            writer.write_row(std::make_tuple(episode, reward));\n",
    "            std::cout<<\"At episode: \"<<episode<<\" total reward: \"<<reward<<std::endl;\n",
    "\n",
    "            if(episode == N_ITERATIONS - 1){\n",
    "                auto& table = sarsalearner.get_table();\n",
    "                table.save_to_csv(\"table_rewards\" + std::to_string(episode) + \".csv\");\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    catch(std::exception& e){\n",
    "\n",
    "        std::cerr<<e.what()<<std::endl;\n",
    "    }\n",
    "    catch(...){\n",
    "\n",
    "        std::cerr<<\"Unknown exception occured\"<<std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"results\"></a> Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "...\n",
    "\n",
    "Taking action: SOUTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 27 and action: SOUTH -1\n",
    "\tNext action: NORTH\n",
    "\tNext state: 15\n",
    "\tSetting for state: 27 and action: SOUTH to value: -1\n",
    "\tAt iteration: 215\n",
    "\tCurrent state: 15\n",
    "\tTaking action: NORTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 15 and action: NORTH -1\n",
    "\tNext action: SOUTH\n",
    "\tNext state: 27\n",
    "\tSetting for state: 15 and action: NORTH to value: -1\n",
    "\tAt iteration: 216\n",
    "\tCurrent state: 27\n",
    "\tTaking action: SOUTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 27 and action: SOUTH -1\n",
    "\tNext action: NORTH\n",
    "\tNext state: 15\n",
    "\tSetting for state: 27 and action: SOUTH to value: -1\n",
    "\tAt iteration: 217\n",
    "\tCurrent state: 15\n",
    "\tTaking action: NORTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 15 and action: NORTH -1\n",
    "\tNext action: SOUTH\n",
    "\tNext state: 27\n",
    "\tSetting for state: 15 and action: NORTH to value: -1\n",
    "\tAt iteration: 218\n",
    "\tCurrent state: 27\n",
    "\tTaking action: SOUTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 27 and action: SOUTH -1\n",
    "\tNext action: NORTH\n",
    "\tNext state: 15\n",
    "\tSetting for state: 27 and action: SOUTH to value: -1\n",
    "\tAt iteration: 219\n",
    "\tCurrent state: 15\n",
    "\tTaking action: NORTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 15 and action: NORTH -1\n",
    "\tNext action: SOUTH\n",
    "\tNext state: 27\n",
    "\tSetting for state: 15 and action: NORTH to value: -1\n",
    "\tAt iteration: 220\n",
    "\tCurrent state: 27\n",
    "\tTaking action: SOUTH\n",
    "\tReward received: -1\n",
    "\tCurrent value for state: 27 and action: SOUTH -1\n",
    "\tNext action: NORTH\n",
    "\tRecalculated next action: SOUTH\n",
    "\tNext state: 15\n",
    "\tSetting for state: 27 and action: SOUTH to value: -1\n",
    "\tAt iteration: 221\n",
    "\tCurrent state: 15\n",
    "\tTaking action: SOUTH\n",
    "WORLD FINISHED AT STATE: 15 AND ACTION: SOUTH\n",
    "At episode: 498 total reward: -220\n",
    "At episode: 499\n",
    "\tAt iteration: 1\n",
    "\tCurrent state: 0\n",
    "\tTaking action: SOUTH\n",
    "WORLD FINISHED AT STATE: 0 AND ACTION: SOUTH\n",
    "At episode: 499 total reward: 0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following images shown the sum of rewards achieved by the algorithm for various values of $\\epsilon$. We see that as $\\epsilon$ is increased more exploration occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"e_0_1.png\"\n",
    "     alt=\"Total Rewards\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"e_0_2.png\"\n",
    "     alt=\"Total Rewards\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"e_0_3.png\"\n",
    "     alt=\"Total Rewards\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"source_code\"></a> Source Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../exe.cpp\">exe.cpp</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
